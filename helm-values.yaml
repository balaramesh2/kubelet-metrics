NAME: weka-operator
LAST DEPLOYED: Mon Aug 18 09:29:15 2025
NAMESPACE: weka-operator-system
STATUS: deployed
REVISION: 1
CHART: weka-operator
VERSION: v1.6.0
APP_VERSION: 4.2.7.815-d3e193ee4ceb15033f5207b466815c3c
TEST SUITE: None
USER-SUPPLIED VALUES:
imagePullSecret: quay-io-robot-secret
otelExporterOtlpEndpoint: https://otelcollector.rnd.weka.io:4317

COMPUTED VALUES:
cleanupClientsOnNodeSelectorMismatch: false
cleanupContainersOnTolerationsMismatch: false
cleanupOnNodeSelectorMismatch: false
cleanupRemovedNodes: false
debugSleep: 3
deployController: true
deploymentIdentifier: ""
dnsPolicy:
  hostNetwork: ""
  k8sNetwork: ""
enableClusterApi: false
enableLeaderElection: true
evictContainerOnDeletion: false
gkeCompatibility:
  disableDriverSigning: false
  gkeServiceAccountSecret: ""
  hugepageConfiguration:
    enabled: false
    hugepageCount: 4000
    hugepageSize: 2M
healthProbeBindAddress: :8081
image:
  repository: quay.io/weka.io/weka-operator
  tag: ""
imagePullSecret: quay-io-robot-secret
kubeExecTimeout: 5m
localDataPvc: ""
localDataStorageClass: ""
logging:
  level: 0
  timeOnly: true
maintenanceImage: busybox
manager:
  labels: {}
  nodeSelector: {}
  resources:
    limits:
      cpu: 1000m
      memory: 4096Mi
    requests:
      cpu: 250m
      memory: 64Mi
  tolerations: []
maxWorkers:
  wekaClient: 5
  wekaCluster: 5
  wekaContainer: 50
  wekaManualOperation: 5
  wekaPolicy: 5
metrics:
  clusters:
    enabled: true
    pollingRate: 60s
  containers:
    enabled: true
    pollingRate: 60s
    requestsTimeouts:
      getContainerInfo: 10s
      register: 3s
nodeAgent:
  nodeSelector: {}
  persistencePaths: /opt/k8s-weka
  resources:
    limits:
      cpu: 1000m
      memory: 1024Mi
    requests:
      cpu: 50m
      memory: 64Mi
  tolerations: []
nodeAgentMetricsBindAddress: :8090
ocpCompatibility:
  driverToolkitImageBaseUrl: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  driverToolkitSecretName: null
  hugepageConfiguration:
    enabled: false
    hugepageSize: 2M
    hugepagesCount: 4000
    machineConfigNodeLabel: worker
    nodeSelector:
      matchLabels:
      - node-role.kubernetes.io/worker: ""
operatorMetricsBindAddress: 127.0.0.1:8080
otelExporterOtlpEndpoint: https://otelcollector.rnd.weka.io:4317
podMonitor:
  enabled: true
prefix: weka-operator
reconcileTimeout: 30m
signDrivesImage: quay.io/weka.io/weka-sign-tool:v0.1.1-pciutils
skipAuxNoScheduleToleration: false
skipClientNoScheduleToleration: false
skipClientsTolerationValidation: false
skipUnhealthyToleration: false
tolerations: []
upgrade:
  computeThresholdPercent: 90
  driveThresholdPercent: 90
  maxDeactivatingContainersPercent: 10
version: ""
wekaAllocZombieDeleteAfter: 5m
wekahome:
  allowInsecureTLS: false
  cacertSecret: ""
  enableStats: true
  endpoint: https://api.home.weka.io

HOOKS:
MANIFEST:
---
# Source: weka-operator/templates/maintenance_service_account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: serviceaccount
    app.kubernetes.io/instance: controller-manager
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-maintenance"
---
# Source: weka-operator/templates/service_account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: serviceaccount
    app.kubernetes.io/instance: controller-manager
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-controller-manager"
---
# Source: weka-operator/templates/ocp_versions_configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ocp-driver-toolkit-images
data:
  414.92.202404162000-0: "259cbd840454c6d9030c21a5d24be0599abc4941cdd525a80f6eeb5d67e7908c"
  414.92.202404231906-0: "20dce872df7c233a34179b4356acc1c6cbd80c56cff053fa437de3b6595f9710"
  414.92.202404301839-0: "20dce872df7c233a34179b4356acc1c6cbd80c56cff053fa437de3b6595f9710"
  415.92.202402201450-0: "cd0ea5d8ec43c5b03bf362e0b595bafe3e97e222d4344a851453ebe8770df135"
  415.92.202403061641-0: "9d974e00ebe924fbd03abf03c55d873108a1593b5a5e60f0daf4b867fc5bb1b1"
  415.92.202403080220-0: "9d974e00ebe924fbd03abf03c55d873108a1593b5a5e60f0daf4b867fc5bb1b1"
  415.92.202403191241-0: "abbff60a77f7ac2276dbeef33fb46ed32c9b9eb1c5813260c6383605bed76a08"
  415.92.202403270524-0: "b9cd86347ba410c90b4a34fe9c1b25951e0f0cd38ceca1d3ccd4bae96f084edb"
  415.92.202404161628-0: "be818782c507d76b48f9f37bcf85e5d5311514ff9e6108b047f80bf6331e63f5"
  415.92.202404251009-0: "bae8035c05d095e84c62efcab6202330a98493cab03e091c81a0b792afb5672c"
  415.92.202404302054-0: "bae8035c05d095e84c62efcab6202330a98493cab03e091c81a0b792afb5672c"
  415.92.202405070140-0: "985b72435a7091702a520581eb51ebd439bfe6ff39c33ffaaad7e30b9e321454"
  415.92.202405130844-0: "985b72435a7091702a520581eb51ebd439bfe6ff39c33ffaaad7e30b9e321454"
  415.92.202405201956-0: "934af754e2fbc8ed5deb7c4b22299c6c7b4504e6d8d9fd50fc3ad374616d70a9"
  415.92.202405281402-0: "d493e0bd8959e0d117493453db9c183e8bca34f73eb89b091134a087554fa0e8"
  415.92.202406041802-0: "9d2c61bf746c966f71bc6c6a3797303a7d3bfaef09040dfde85f19383d19681b"
  415.92.202406111137-0: "efa99ae171e7db22aa2d320b7bc78e950db01987889b6a8529e1945670e80792"
  416.94.202406172220-0: "dde3cd6a75d865a476aa7e1cab6fa8d97742401e87e0d514f3042c3a881e301f"
  416.94.202406251923-0: "8ef92caba7bd5d6ab3a139da782bf5651c2a40802eaa33b0c7899a7e897e007b"
  416.94.202407030122-0: "e5e6de7572003ac560f113a0082594a585c49d51801f028f699b15262eff7c02"
  416.94.202407081958-0: "a73204d0c03454b02656801ca4c49cb2d8b0d54645bb90f74622df335c82dce1"
---
# Source: weka-operator/templates/weka_boot_scripts_configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: weka-boot-scripts
data:
  syslog-ng.conf: |
    @version: 3.35
    
    options {
        use_dns(no);
        dns_cache(no);
        keep_hostname(yes);
        create_dirs(yes);
        ts_format(iso);
    };
    
    source s_net {
        unix-stream("/var/run/syslog-ng/syslog-ng.sock");
        unix-dgram("/run/systemd/journal/dev-log", create_dirs(yes));
    };
    
    destination d_stdout {
        file("/dev/stdout"
            template("$ISODATE $MSGHDR | $MSG\n")
        );
    };
    
    destination d_syslog {
        file("/var/log/syslog"
            template("$ISODATE $MSGHDR | $MSG\n")
            template_escape(no)
        );
    };
    
    destination d_error {
        file("/var/log/error"
            template("$ISODATE $MSGHDR | $MSG\n")
            template_escape(no)
        );
    };
    
    filter f_info {
        match(".*(NOTICE|WARN(ING)*|ERR(OR)*|CRIT(ICAL)*|ALERT|EMERG(ENCY)*|FATAL|ASSERT):.*" value("MESSAGE"));
    };
    
    filter f_error {
        match(".*(ERR(OR)*|CRIT(ICAL)*|ALERT|EMERG(ENCY)*|FATAL|ASSERT):.*" value("MESSAGE"));
    };
    
    log {
        source(s_net);
        filter(f_info);
        destination(d_stdout);
    };
    
    log {
        source(s_net);
        destination(d_syslog);
    };
    
    log {
        source(s_net);
        filter(f_error);
        destination(d_error);
    };
    
    
  run-weka-cli.sh: |
    #!/bin/bash
    
    set -o pipefail
    set -e
    
    if [[ -f /var/run/secrets/weka-operator/operator-user/username ]]; then
      export WEKA_USERNAME=`cat /var/run/secrets/weka-operator/operator-user/username`
      export WEKA_PASSWORD=`cat /var/run/secrets/weka-operator/operator-user/password`
      export WEKA_ORG=`cat /var/run/secrets/weka-operator/operator-user/org`
    fi
    
    # comes either out of pod spec on repeat run or from resources.json on first run
    if [[ "$PORT" == "0" ]]; then
      if [[ -f /opt/weka/k8s-runtime/vars/port ]]; then
        export PORT=`cat /opt/weka/k8s-runtime/vars/port`
        export WEKA_PORT=`cat /opt/weka/k8s-runtime/vars/port`
      fi
    fi
    
    if [[ "$AGENT_PORT" == "0" ]]; then
      if [[ -f /opt/weka/k8s-runtime/vars/agent_port ]]; then
        export AGENT_PORT=`cat /opt/weka/k8s-runtime/vars/agent_port`
      fi
    fi
    
    
    /usr/bin/weka "$@"
    
  weka_runtime.py: |
    import base64
    import fcntl
    import json
    import logging
    import os
    import re
    import socket
    import struct
    import subprocess
    import sys
    import threading
    import time
    from dataclasses import dataclass
    from functools import lru_cache, partial
    from os.path import exists
    from textwrap import dedent
    from typing import List, Optional, Tuple
    
    @dataclass
    class SignOptions:
        allowEraseWekaPartitions: bool = False
        allowEraseNonWekaPartitions: bool = False
        allowNonEmptyDevice: bool = False
        skipTrimFormat: bool = False
    
    MODE = os.environ.get("MODE")
    assert MODE != ""
    NUM_CORES = int(os.environ.get("CORES", 0))
    CORE_IDS = os.environ.get("CORE_IDS", "auto")
    CPU_POLICY = os.environ.get("CPU_POLICY", "auto")
    NAME = os.environ["NAME"]
    NETWORK_DEVICE = os.environ.get("NETWORK_DEVICE", "")
    SUBNETS = os.environ.get("SUBNETS", "")
    PORT = os.environ.get("PORT", "")
    AGENT_PORT = os.environ.get("AGENT_PORT", "")
    RESOURCES = {}  # to be populated at later stage
    MEMORY = os.environ.get("MEMORY", "")
    JOIN_IPS = os.environ.get("JOIN_IPS", "")
    DIST_SERVICE = os.environ.get("DIST_SERVICE")
    OS_DISTRO = ""
    OS_BUILD_ID = ""
    DISCOVERY_SCHEMA = 1
    INSTRUCTIONS = os.environ.get("INSTRUCTIONS", "")
    NODE_NAME = os.environ["NODE_NAME"]
    POD_ID = os.environ.get("POD_ID", "")
    FAILURE_DOMAIN = os.environ.get("FAILURE_DOMAIN", None)
    MACHINE_IDENTIFIER = os.environ.get("MACHINE_IDENTIFIER", None)
    NET_GATEWAY = os.environ.get("NET_GATEWAY", None)
    IS_IPV6 = os.environ.get("IS_IPV6", "false") == "true"
    MANAGEMENT_IPS = []  # to be populated at later stage
    UDP_MODE = os.environ.get("UDP_MODE", "false") == "true"
    DUMPER_CONFIG_MODE = os.environ.get("DUMPER_CONFIG_MODE", "auto")
    
    KUBERNETES_DISTRO_OPENSHIFT = "openshift"
    KUBERNETES_DISTRO_GKE = "gke"
    OS_NAME_GOOGLE_COS = "cos"
    OS_NAME_REDHAT_COREOS = "rhcos"
    
    MAX_TRACE_CAPACITY_GB = os.environ.get("MAX_TRACE_CAPACITY_GB", 10)
    ENSURE_FREE_SPACE_GB = os.environ.get("ENSURE_FREE_SPACE_GB", 20)
    
    WEKA_CONTAINER_ID = os.environ.get("WEKA_CONTAINER_ID", "")
    WEKA_PERSISTENCE_DIR = "/host-binds/opt-weka"
    WEKA_PERSISTENCE_MODE = os.environ.get("WEKA_PERSISTENCE_MODE", "local")
    WEKA_PERSISTENCE_GLOBAL_DIR = "/opt/weka-global-persistence"
    if WEKA_PERSISTENCE_MODE == "global":
        WEKA_PERSISTENCE_DIR = os.path.join(WEKA_PERSISTENCE_GLOBAL_DIR, "containers", WEKA_CONTAINER_ID)
    
    WEKA_COS_ALLOW_HUGEPAGE_CONFIG = True if os.environ.get("WEKA_COS_ALLOW_HUGEPAGE_CONFIG", "false") == "true" else False
    WEKA_COS_ALLOW_DISABLE_DRIVER_SIGNING = True if os.environ.get("WEKA_COS_ALLOW_DISABLE_DRIVER_SIGNING",
                                                                   "false") == "true" else False
    WEKA_COS_GLOBAL_HUGEPAGE_SIZE = os.environ.get("WEKA_COS_GLOBAL_HUGEPAGE_SIZE", "2M").lower()
    WEKA_COS_GLOBAL_HUGEPAGE_COUNT = int(os.environ.get("WEKA_COS_GLOBAL_HUGEPAGE_COUNT", 4000))
    
    AWS_VENDOR_ID = "1d0f"
    AWS_DEVICE_ID = "cd01"
    AUTO_REMOVE_TIMEOUT = int(os.environ.get("AUTO_REMOVE_TIMEOUT", "0"))
    
    # for client dynamic port allocation
    BASE_PORT = os.environ.get("BASE_PORT", "")
    PORT_RANGE = os.environ.get("PORT_RANGE", "0")
    WEKA_CONTAINER_PORT_SUBRANGE = 100
    MAX_PORT = 65535
    
    # Define global variables
    exiting = 0
    
    # Formatter with channel name
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Define handlers for stdout and stderr
    stdout_handler = logging.StreamHandler(sys.stdout)
    stdout_handler.setLevel(logging.DEBUG)
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(logging.WARNING)
    
    # Basic configuration
    logging.basicConfig(
        level=logging.DEBUG,  # Global minimum logging level
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Include timestamp
        handlers=[stdout_handler, stderr_handler]
    )
    
    
    async def sign_drives_by_pci_info(vendor_id: str, device_id: str, options: dict) -> List[str]:
        logging.info("Signing drives. Vendor ID: %s, Device ID: %s", vendor_id, device_id)
    
        if not vendor_id or not device_id:
            raise ValueError("Vendor ID and Device ID are required")
    
        cmd = f"lspci -d {vendor_id}:{device_id}" + " | sort | awk '{print $1}'"
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            return
    
        signed_drives = []
        pci_devices = stdout.decode().strip().split()
        for pci_device in pci_devices:
            device = f"/dev/disk/by-path/pci-0000:{pci_device}-nvme-1"
            try:
                await sign_device_path(device, options)
                signed_drives.append(device)
            except SignException as e:
                logging.error(str(e))
                continue
        return signed_drives
    
    
    async def sign_not_mounted(options: dict) -> List[str]:
        """
        [root@wekabox18 sdc] 2024-08-21 19:14:58 $ cat /run/udev/data/b`cat /sys/block/sdc/dev` | grep ID_SERIAL=
            E:ID_SERIAL=TOSHIBA_THNSN81Q92CSE_86DS107ATB4V
        :return:
        """
        logging.info("Signing drives not mounted")
        stdout, stderr, ec = await run_command("lsblk -o NAME,TYPE,MOUNTPOINT", capture_stdout=True)
        if ec != 0:
            return
        lines = stdout.decode().strip().split("\n")
        signed_drives = []
        logging.info(f"Found drives: {lines}")
        for line in lines[1:]:
            parts = line.split()
            logging.info(f"Processing line: {parts}")
            if parts[1] == "disk" and len(parts) < 3:
                device = f"/dev/{parts[0]}"
                try:
                    await sign_device_path(device, options)
                    signed_drives.append(device)
                except SignException as e:
                    logging.error(str(e))
                    continue
        return signed_drives
    
    
    async def sign_device_paths(devices_paths, options) -> List[str]:
        signed_drives = []
        for device_path in devices_paths:
            try:
                await sign_device_path(device_path, options)
                signed_drives.append(device_path)
            except SignException as e:
                logging.error(str(e))
                continue
        return signed_drives
    
    
    class SignException(Exception):
        pass
    
    
    async def sign_device_path(device_path, options: SignOptions):
        logging.info(f"Signing drive {device_path}")
        params = []
        if options.allowEraseWekaPartitions:
            params.append("--allow-erase-weka-partitions")
        if options.allowEraseNonWekaPartitions:
            params.append("--allow-erase-non-weka-partitions")
        if options.allowNonEmptyDevice:
            params.append("--allow-non-empty-device")
        if options.skipTrimFormat:
            params.append("--skip-trim-format")
    
        stdout, stderr, ec = await run_command(f"nsenter --target=1 --mount /weka-sign-drive {' '.join(params)} -- {device_path}")
        if ec != 0:
            err = f"Failed to sign drive {device_path}: {stderr}"
            raise SignException(err)
    
    
    async def sign_drives(instruction: dict) -> List[str]:
        type = instruction['type']
        options = SignOptions(**instruction.get('options', {})) if instruction.get('options') else SignOptions()
    
        if type == "aws-all":
            return await sign_drives_by_pci_info(
                vendor_id=AWS_VENDOR_ID,
                device_id=AWS_DEVICE_ID,
                options=options
            )
        elif type == "device-identifiers":
            return await sign_drives_by_pci_info(
                vendor_id=instruction.get('pciDevices', {}).get('vendorId'),
                device_id=instruction.get('pciDevices', {}).get('deviceId'),
                options=options
            )
        elif type == "all-not-root":
            return await sign_not_mounted(options)
        elif type == "device-paths":
            return await sign_device_paths(instruction['devicePaths'], options)
        else:
            raise ValueError(f"Unknown instruction type: {type}")
    
    
    async def force_resign_drives_by_paths(devices_paths: List[str]):
        logging.info("Force resigning drives by paths: %s", devices_paths)
        signed_drives = []
        options = SignOptions(allowEraseWekaPartitions=True)
        for device_path in devices_paths:
            try:
                await sign_device_path(device_path, options)
                signed_drives.append(device_path)
            except SignException as e:
                logging.error(str(e))
                continue
        write_results(dict(
            err=None,
            drives=signed_drives
        ))
    
    
    async def force_resign_drives_by_serials(serials: List[str]):
        logging.info("Force resigning drives by serials: %s", serials)
        device_paths = []
        for serial in serials:
            device_path = await get_block_device_path_by_serial(serial)
            device_paths.append(device_path)
    
        await force_resign_drives_by_paths(device_paths)
    
    
    async def get_block_device_path_by_serial(serial: str):
        logging.info(f"Getting block device path by serial {serial}")
        stdout, stderr, ec = await run_command(
            "lsblk -dpno NAME | grep -w $(basename $(ls -la /dev/disk/by-id/ | grep -m 1 " + serial + " | awk '{print $NF}'))")
        if ec != 0:
            logging.error(f"Failed to get block device path by serial {serial}: {stderr}")
            return
        device_path = stdout.decode().strip()
        return device_path
    
    
    async def discover_drives():
        drives = await find_weka_drives()
        write_results(dict(
            err=None,
            drives=drives,
        ))
    
    
    async def find_weka_drives():
        drives = []
        # ls /dev/disk/by-path/pci-0000\:03\:00.0-scsi-0\:0\:3\:0  | ssd
    
        devices_by_id = subprocess.check_output("ls /dev/disk/by-id/", shell=True).decode().strip().split()
        devices_by_path = subprocess.check_output("ls /dev/disk/by-path/", shell=True).decode().strip().split()
    
        part_names = []
    
        def resolve_to_part_name():
            # TODO: A bit dirty, consolidate paths
            for device in devices_by_path:
                try:
                    part_name = subprocess.check_output(f"basename $(readlink -f /dev/disk/by-path/{device})",
                                                        shell=True).decode().strip()
                except subprocess.CalledProcessError:
                    logging.error(f"Failed to get part name for {device}")
                    continue
                part_names.append(part_name)
            for device in devices_by_id:
                try:
                    part_name = subprocess.check_output(f"basename $(readlink -f /dev/disk/by-id/{device})",
                                                        shell=True).decode().strip()
                    if part_name in part_names:
                        continue
                except subprocess.CalledProcessError:
                    logging.error(f"Failed to get part name for {device}")
                    continue
                part_names.append(part_name)
    
        resolve_to_part_name()
    
        logging.info(f"All found in kernel block devices: {part_names}")
        for part_name in part_names:
            try:
                type_id = subprocess.check_output(f"blkid -s PART_ENTRY_TYPE -o value -p /dev/{part_name}",
                                                  shell=True).decode().strip()
            except subprocess.CalledProcessError:
                logging.error(f"Failed to get PART_ENTRY_TYPE for {part_name}")
                continue
    
            if type_id == "993ec906-b4e2-11e7-a205-a0a8cd3ea1de":
                # TODO: Read and populate actual weka guid here
                weka_guid = ""
                # resolve block_device to serial id
                pci_device_path = subprocess.check_output(f"readlink -f /sys/class/block/{part_name}",
                                                          shell=True).decode().strip()
                if "nvme" in part_name:
                    # 3 directories up is the serial id
                    serial_id_path = "/".join(pci_device_path.split("/")[:-2]) + "/serial"
                    serial_id = subprocess.check_output(f"cat {serial_id_path}", shell=True).decode().strip()
                    device_path = "/dev/" + pci_device_path.split("/")[-2]
                else:
                    device_name = pci_device_path.split("/")[-2]
                    device_path = "/dev/" + device_name
                    dev_index = subprocess.check_output(f"cat /sys/block/{device_name}/dev", shell=True).decode().strip()
                    serial_id_cmd = f"cat /host/run/udev/data/b{dev_index} | grep ID_SERIAL="
                    serial_id = subprocess.check_output(serial_id_cmd, shell=True).decode().strip().split("=")[-1]
    
                drives.append({
                    "partition": "/dev/" + part_name,
                    "block_device": device_path,
                    "serial_id": serial_id,
                    "weka_guid": weka_guid
                })
    
        return drives
    
    
    def is_google_cos():
        return OS_DISTRO == OS_NAME_GOOGLE_COS
    
    
    def is_rhcos():
        return OS_DISTRO == OS_NAME_REDHAT_COREOS
    
    
    def wait_for_syslog():
        while not os.path.isfile('/var/run/syslog-ng.pid'):
            time.sleep(0.1)
            print("Waiting for syslog-ng to start")
    
    
    def wait_for_agent():
        while not os.path.isfile('/var/run/weka-agent.pid'):
            time.sleep(1)
            print("Waiting for weka-agent to start")
    
    
    async def ensure_drivers():
        logging.info("waiting for drivers")
        drivers = "wekafsio wekafsgw mpin_user".split()
        if not is_google_cos():
            drivers.append("igb_uio")
            if version_params.get('uio_pci_generic') is not False:
                drivers.append("uio_pci_generic")
        driver_mode = await is_legacy_driver_cmd()
        logging.info(f"validating drivers in mode {MODE}, driver mode: {driver_mode}")
        if not await is_legacy_driver_cmd() and MODE in ["client", "s3",
                                                         "nfs"]:  # we are not using legacy driver on backends, as it should not be validating specific versions, so just lsmoding
            while not exiting:
                version = await get_weka_version()
                stdout, stderr, ec = await run_command(f"weka driver ready --without-agent --version {version}")
                if ec != 0:
                    with open("/tmp/weka-drivers.log_tmp", "w") as f:
                        f.write("weka-drivers-loading")
                        logging.warning(f"Drivers are not loaded, waiting for them")
                    os.rename("/tmp/weka-drivers.log_tmp", "/tmp/weka-drivers.log")
                    logging.error(f"Failed to validate drivers {stderr.decode('utf-8')}: exc={ec}")
                    await asyncio.sleep(1)
                    continue
                logging.info("drivers are ready")
                break
        else:
            for driver in drivers:
                while True:
                    stdout, stderr, ec = await run_command(f"lsmod | grep -w {driver}")
                    if ec == 0:
                        break
                    # write driver name into /tmp/weka-drivers.log
                    logging.info(f"Driver {driver} not loaded, waiting for it")
                    with open("/tmp/weka-drivers.log_tmp", "w") as f:
                        logging.warning(f"Driver {driver} not loaded, waiting for it")
                        f.write(driver)
                    os.rename("/tmp/weka-drivers.log_tmp", "/tmp/weka-drivers.log")
                    await asyncio.sleep(1)
                    continue
    
        with open("/tmp/weka-drivers.log_tmp", "w") as f:
            f.write("")
        os.rename("/tmp/weka-drivers.log_tmp", "/tmp/weka-drivers.log")
        logging.info("All drivers loaded successfully")
    
    
    # This atrocities should be replaced by new weka driver build/publish/download/install functionality
    VERSION_TO_DRIVERS_MAP_WEKAFS = {
        "4.3.1.29791-9f57657d1fb70e71a3fb914ff7d75eee-dev": dict(
            wekafs="cc9937c66eb1d0be-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.2.560-842278e2dca9375f84bd3784a4e7515c-dev3": dict(
            wekafs="1acd22f9ddbda67d-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.2.560-842278e2dca9375f84bd3784a4e7515c-dev4": dict(
            wekafs="1acd22f9ddbda67d-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.2.560-842278e2dca9375f84bd3784a4e7515c-dev5": dict(
            wekafs="1acd22f9ddbda67d-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.2.783-f5fe2ec58286d9fa8fc033f920e6c842-dev": dict(
            wekafs="1cb1639d52a2b9ca-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.3.28-k8s-alpha-dev": dict(
            wekafs="1cb1639d52a2b9ca-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.3.28-k8s-alpha-dev2": dict(
            wekafs="1cb1639d52a2b9ca-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.3.28-k8s-alpha-dev3": dict(
            wekafs="1cb1639d52a2b9ca-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.2.783-f5fe2ec58286d9fa8fc033f920e6c842-dev2": dict(
            wekafs="1cb1639d52a2b9ca-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.3.2.783-f5fe2ec58286d9fa8fc033f920e6c842-dev3": dict(
            wekafs="1cb1639d52a2b9ca-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="6b519d501ea82063",
        ),
        "4.2.7.64-k8so-beta.10": dict(
            wekafs="1.0.0-995f26b334137fd78d57c264d5b19852-GW_aedf44a11ca66c7bb599f302ae1dff86",
        ),
        "4.2.10.1693-251d3172589e79bd4960da8031a9a693-dev": dict(  # dev 4.2.7-based version
            wekafs="1.0.0-995f26b334137fd78d57c264d5b19852-GW_aedf44a11ca66c7bb599f302ae1dff86",
        ),
        "4.2.10.1290-e552f99e92504c69126da70e1740f6e4-dev": dict(
            wekafs="1.0.0-c50570e208c935e9129c9054140ab11a-GW_aedf44a11ca66c7bb599f302ae1dff86",
        ),
        "4.2.10-k8so.0": dict(
            wekafs="1.0.0-c50570e208c935e9129c9054140ab11a-GW_aedf44a11ca66c7bb599f302ae1dff86",
        ),
        "4.2.10.1671-363e1e8fcfb1290e061815445e973310-dev": dict(
            wekafs="1.0.0-c50570e208c935e9129c9054140ab11a-GW_aedf44a11ca66c7bb599f302ae1dff86",
        ),
        "4.3.3": dict(
            wekafs="cbd05f716a3975f7-GW_556972ab1ad2a29b0db5451e9db18748",
            uio_pci_generic=False,
            dependencies="7955984e4bce9d8b",
            weka_drivers_handling=False,
        ),
    }
    # WEKA_DRIVER_VERSION_OPTIONS = [
    #     "1.0.0-c50570e208c935e9129c9054140ab11a-GW_aedf44a11ca66c7bb599f302ae1dff86",
    #     "1.0.0-995f26b334137fd78d57c264d5b19852-GW_aedf44a11ca66c7bb599f302ae1dff86",
    # ]
    IGB_UIO_DRIVER_VERSION = "weka1.0.2"
    MPIN_USER_DRIVER_VERSION = "1.0.1"
    UIO_PCI_GENERIC_DRIVER_VERSION = "5f49bb7dc1b5d192fb01b442b17ddc0451313ea2"
    DEFAULT_DEPENDENCY_VERSION = "1.0.0-024f0fdaa33ec66087bc6c5631b85819"
    
    IMAGE_NAME = os.environ.get("IMAGE_NAME")
    DEFAULT_PARAMS = dict(
        weka_drivers_handling=True,
        uio_pci_generic=False,
    )
    version_params = VERSION_TO_DRIVERS_MAP_WEKAFS.get(os.environ.get("IMAGE_NAME").split(":")[-1], DEFAULT_PARAMS)
    if "4.2.7.64-s3multitenancy." in IMAGE_NAME:
        version_params = dict(
            wekafs="1.0.0-995f26b334137fd78d57c264d5b19852-GW_aedf44a11ca66c7bb599f302ae1dff86",
            mpin_user="f8c7f8b24611c2e458103da8de26d545",
            igb_uio="b64e22645db30b31b52f012cc75e9ea0",
            uio_pci_generic="1.0.0-929f279ce026ddd2e31e281b93b38f52",
        )
    assert version_params
    
    WEKA_DRIVERS_HANDLING = True if version_params.get("weka_drivers_handling") else False
    
    # Implement the rest of your logic here
    import asyncio
    import os
    import signal
    
    loop = asyncio.get_event_loop()
    
    
    async def get_weka_version():
        files = os.listdir("/opt/weka/dist/release")
        assert len(files) == 1, Exception(f"More then one release found: {files}")
        version = files[0].partition(".spec")[0]
        return version
    
    
    async def load_drivers():
        def should_skip_uio_pci_generic():
            return version_params.get('uio_pci_generic') is False or should_skip_uio()
    
        def should_skip_uio():
            return is_google_cos()
    
        def should_skip_igb_uio():
            return should_skip_uio()
    
        if is_rhcos():
            if os.path.isdir("/hostpath/lib/modules"):
                os.system("cp -r /hostpath/lib/modules/* /lib/modules/")
    
        if not WEKA_DRIVERS_HANDLING:
            # LEGACY MODE
            weka_driver_version = version_params.get('wekafs')
            download_cmds = [
                (f"mkdir -p /opt/weka/dist/drivers", "creating drivers directory"),
                (
                    f"curl -kfo /opt/weka/dist/drivers/weka_driver-wekafsgw-{weka_driver_version}-$(uname -r).$(uname -m).ko {DIST_SERVICE}/dist/v1/drivers/weka_driver-wekafsgw-{weka_driver_version}-$(uname -r).$(uname -m).ko",
                    "downloading wekafsgw driver"),
                (
                    f"curl -kfo /opt/weka/dist/drivers/weka_driver-wekafsio-{weka_driver_version}-$(uname -r).$(uname -m).ko {DIST_SERVICE}/dist/v1/drivers/weka_driver-wekafsio-{weka_driver_version}-$(uname -r).$(uname -m).ko",
                    "downloading wekafsio driver"),
                (
                    f"curl -kfo /opt/weka/dist/drivers/mpin_user-{MPIN_USER_DRIVER_VERSION}-$(uname -r).$(uname -m).ko {DIST_SERVICE}/dist/v1/drivers/mpin_user-{MPIN_USER_DRIVER_VERSION}-$(uname -r).$(uname -m).ko",
                    "downloading mpin_user driver")
            ]
            if not should_skip_igb_uio():
                download_cmds.append((
                    f"curl -kfo /opt/weka/dist/drivers/igb_uio-{IGB_UIO_DRIVER_VERSION}-$(uname -r).$(uname -m).ko {DIST_SERVICE}/dist/v1/drivers/igb_uio-{IGB_UIO_DRIVER_VERSION}-$(uname -r).$(uname -m).ko",
                    "downloading igb_uio driver"))
            if not should_skip_uio_pci_generic():
                download_cmds.append((
                    f"curl -kfo /opt/weka/dist/drivers/uio_pci_generic-{UIO_PCI_GENERIC_DRIVER_VERSION}-$(uname -r).$(uname -m).ko {DIST_SERVICE}/dist/v1/drivers/uio_pci_generic-{UIO_PCI_GENERIC_DRIVER_VERSION}-$(uname -r).$(uname -m).ko",
                    "downloading uio_pci_generic driver"))
    
            load_cmds = [
                (
                    f"lsmod | grep -w wekafsgw || insmod /opt/weka/dist/drivers/weka_driver-wekafsgw-{weka_driver_version}-$(uname -r).$(uname -m).ko",
                    "loading wekafsgw driver"),
                (
                    f"lsmod | grep -w wekafsio || insmod /opt/weka/dist/drivers/weka_driver-wekafsio-{weka_driver_version}-$(uname -r).$(uname -m).ko",
                    "loading wekafsio driver"),
                (
                    f"lsmod | grep -w mpin_user || insmod /opt/weka/dist/drivers/mpin_user-{MPIN_USER_DRIVER_VERSION}-$(uname -r).$(uname -m).ko",
                    "loading mpin_user driver")
            ]
            if not should_skip_uio():
                load_cmds.append((f"lsmod | grep -w uio || modprobe uio", "loading uio driver"))
            if not should_skip_igb_uio():
                load_cmds.append((
                    f"lsmod | grep -w igb_uio || insmod /opt/weka/dist/drivers/igb_uio-{IGB_UIO_DRIVER_VERSION}-$(uname -r).$(uname -m).ko",
                    "loading igb_uio driver"))
    
        else:
            # list directory /opt/weka/dist/version
            # assert single json file and take json filename
            version = await get_weka_version()
            download_cmds = [
                (f"weka driver download --from '{DIST_SERVICE}' --without-agent --version {version}", "Downloading drivers")
            ]
            load_cmds = [
                (f"rmmod wekafsio || echo could not unload old wekafsio driver, still trying to proceed",
                 "unloading wekafsio"),
                (f"rmmod wekafsgw || echo could not unload old wekafsgw driver, still trying to proceed",
                 "unloading wekafsgw"),
                (f"weka driver install --without-agent --version {version}", "loading drivers"),
            ]
        if not should_skip_uio_pci_generic():
            load_cmds.append((
                f"lsmod | grep -w uio_pci_generic || insmod /opt/weka/dist/drivers/uio_pci_generic-{UIO_PCI_GENERIC_DRIVER_VERSION}-$(uname -r).$(uname -m).ko",
                "loading uio_pci_generic driver"))
    
        # load vfio-pci if not loaded and iommu groups are present
        cmd = '[ "$(ls -A /sys/kernel/iommu_groups/)" ] && lsmod | grep -w vfio_pci || modprobe vfio-pci'
        _, stderr, ec = await run_command(cmd)
        if ec != 0:
            logging.error(f"Failed to load vfio-pci {stderr.decode('utf-8')}: exc={ec}, last command: {cmd}")
            raise Exception(f"Failed to load vfio-pci: {stderr}")
    
        logging.info("Downloading and loading drivers")
        for cmd, desc in download_cmds + load_cmds:
            logging.info(f"Driver loading step: {desc}")
            stdout, stderr, ec = await run_command(cmd)
            if ec != 0:
                logging.error(f"Failed to load drivers {stderr.decode('utf-8')}: exc={ec}, last command: {cmd}")
                raise Exception(f"Failed to load drivers: {stderr.decode('utf-8')}")
        logging.info("All drivers loaded successfully")
    
    
    async def copy_drivers():
        if WEKA_DRIVERS_HANDLING:
            return
    
        weka_driver_version = version_params.get('wekafs')
        assert weka_driver_version
    
        stdout, stderr, ec = await run_command(dedent(f"""
          mkdir -p /opt/weka/dist/drivers
          cp /opt/weka/data/weka_driver/{weka_driver_version}/$(uname -r)/wekafsio.ko /opt/weka/dist/drivers/weka_driver-wekafsio-{weka_driver_version}-$(uname -r).$(uname -m).ko
          cp /opt/weka/data/weka_driver/{weka_driver_version}/$(uname -r)/wekafsgw.ko /opt/weka/dist/drivers/weka_driver-wekafsgw-{weka_driver_version}-$(uname -r).$(uname -m).ko
    
          cp /opt/weka/data/igb_uio/{IGB_UIO_DRIVER_VERSION}/$(uname -r)/igb_uio.ko /opt/weka/dist/drivers/igb_uio-{IGB_UIO_DRIVER_VERSION}-$(uname -r).$(uname -m).ko
          cp /opt/weka/data/mpin_user/{MPIN_USER_DRIVER_VERSION}/$(uname -r)/mpin_user.ko /opt/weka/dist/drivers/mpin_user-{MPIN_USER_DRIVER_VERSION}-$(uname -r).$(uname -m).ko
          {"" if version_params.get('uio_pci_generic') == False else f"cp /opt/weka/data/uio_generic/{UIO_PCI_GENERIC_DRIVER_VERSION}/$(uname -r)/uio_pci_generic.ko /opt/weka/dist/drivers/uio_pci_generic-{UIO_PCI_GENERIC_DRIVER_VERSION}-$(uname -r).$(uname -m).ko"}
        """))
        if ec != 0:
            logging.info(f"Failed to copy drivers post build {stderr}: exc={ec}")
            raise Exception(f"Failed to copy drivers post build: {stderr}")
        logging.info("done copying drivers")
    
    
    async def cos_build_drivers():
        weka_driver_version = version_params["wekafs"]
        weka_driver_file_version = weka_driver_version.rsplit("-", 1)[0]
        mpin_driver_version = version_params["mpin_user"]
        igb_uio_driver_version = version_params["igb_uio"]
        uio_pci_generic_driver_version = version_params.get("uio_pci_generic", "1.0.0-929f279ce026ddd2e31e281b93b38f52")
        weka_driver_squashfs = f'/opt/weka/dist/image/weka-driver-{weka_driver_file_version}.squashfs'
        mpin_driver_squashfs = f'/opt/weka/dist/image/driver-mpin-user-{mpin_driver_version}.squashfs'
        igb_uio_driver_squashfs = f'/opt/weka/dist/image/driver-igb-uio-{igb_uio_driver_version}.squashfs'
        uio_pci_driver_squashfs = f'/opt/weka/dist/image/driver-uio-pci-generic-{uio_pci_generic_driver_version}.squashfs'
        logging.info(f"Building drivers for Google Container-Optimized OS release {OS_BUILD_ID}")
        for cmd, desc in [
            (f"apt-get install -y squashfs-tools", "installing squashfs-tools"),
            (f"mkdir -p /opt/weka/data/weka_driver/{weka_driver_version}/$(uname -r)", "downloading weka driver"),
            (f"mkdir -p /opt/weka/data/mpin_user/{MPIN_USER_DRIVER_VERSION}/$(uname -r)", "downloading mpin driver"),
            (f"mkdir -p /opt/weka/data/igb_uio/{IGB_UIO_DRIVER_VERSION}/$(uname -r)", "downloading igb_uio driver"),
            (f"mkdir -p /opt/weka/data/uio_generic/{UIO_PCI_GENERIC_DRIVER_VERSION}/$(uname -r)",
             "downloading uio_pci_generic driver"),
            (f"unsquashfs -i -f -d /opt/weka/data/weka_driver/{weka_driver_version}/$(uname -r) {weka_driver_squashfs}",
             "extracting weka driver"),
            (f"unsquashfs -i -f -d /opt/weka/data/mpin_user/{MPIN_USER_DRIVER_VERSION}/$(uname -r) {mpin_driver_squashfs}",
             "extracting mpin driver"),
            (f"unsquashfs -i -f -d /opt/weka/data/igb_uio/{IGB_UIO_DRIVER_VERSION}/$(uname -r) {igb_uio_driver_squashfs}",
             "extracting igb_uio driver"),
            (
                    f"unsquashfs -i -f -d /opt/weka/data/uio_generic/{UIO_PCI_GENERIC_DRIVER_VERSION}/$(uname -r) {uio_pci_driver_squashfs}",
                    "extracting uio_pci_generic driver"),
            (f"cd /opt/weka/data/weka_driver/{weka_driver_version}/$(uname -r) && /devenv.sh -R {OS_BUILD_ID} -m ",
             "building weka driver"),
            (f"cd /opt/weka/data/mpin_user/{MPIN_USER_DRIVER_VERSION}/$(uname -r) && /devenv.sh -R {OS_BUILD_ID} -m",
             "building mpin driver"),
            (f"cd /opt/weka/data/igb_uio/{IGB_UIO_DRIVER_VERSION}/$(uname -r) && /devenv.sh -R {OS_BUILD_ID} -m",
             "building igb_uio driver"),
            (
                    f"cd /opt/weka/data/uio_generic/{UIO_PCI_GENERIC_DRIVER_VERSION}/$(uname -r) && /devenv.sh -R {OS_BUILD_ID} -m",
                    "building uio_pci_generic driver"),
        ]:
            logging.info(f"COS driver building step: {desc}")
            stdout, stderr, ec = await run_command(cmd)
            if ec != 0:
                logging.error(f"Failed to build drivers {stderr}: exc={ec}, last command: {cmd}")
                raise Exception(f"Failed to build drivers: {stderr}")
    
        logging.info("Done building drivers")
    
    
    def parse_cpu_allowed_list(path="/proc/1/status"):
        with open(path) as file:
            for line in file:
                if line.startswith("Cpus_allowed_list"):
                    return expand_ranges(line.strip().split(":\t")[1])
        return []
    
    
    def expand_ranges(ranges_str):
        ranges = []
        for part in ranges_str.split(','):
            if '-' in part:
                start, end = map(int, part.split('-'))
                ranges.extend(list(range(start, end + 1)))
            else:
                ranges.append(int(part))
        return ranges
    
    
    def read_siblings_list(cpu_index):
        path = f"/sys/devices/system/cpu/cpu{cpu_index}/topology/thread_siblings_list"
        with open(path) as file:
            return expand_ranges(file.read().strip())
    
    
    @dataclass
    class HostInfo:
        kubernetes_distro = 'k8s'
        os = 'unknown'
        os_build_id = ''  # this is either COS build ID OR OpenShift version tag, e.g. 415.92.202406111137-0
    
        def is_rhcos(self):
            return self.os == OS_NAME_REDHAT_COREOS
    
        def is_cos(self):
            return self.os == OS_NAME_GOOGLE_COS
    
    
    def get_host_info():
        raw_data = {}
        ret = HostInfo()
        with open("/hostside/etc/os-release") as file:
            for line in file:
                try:
                    k, v = line.strip().split("=")
                except ValueError:
                    continue
                if v:
                    raw_data[k] = v.strip().replace('"', '')
    
        ret.os = raw_data.get("ID", "")
    
        if ret.is_rhcos():
            ret.kubernetes_distro = KUBERNETES_DISTRO_OPENSHIFT
            ret.os_build_id = raw_data.get("VERSION", "")
    
        elif ret.is_cos():
            ret.kubernetes_distro = KUBERNETES_DISTRO_GKE
            ret.os_build_id = raw_data.get("BUILD_ID", "")
        return ret
    
    
    @lru_cache
    def find_full_cores(n):
        if CORE_IDS != "auto":
            return list(CORE_IDS.split(","))
    
        selected_siblings = []
    
        available_cores = parse_cpu_allowed_list()
        zero_siblings = [] if 0 not in available_cores else read_siblings_list(0)
    
        for cpu_index in available_cores:
            if cpu_index in zero_siblings:
                continue
    
            siblings = read_siblings_list(cpu_index)
            if all(sibling in available_cores for sibling in siblings):
                if any(sibling in selected_siblings for sibling in siblings):
                    continue
                selected_siblings.append(siblings[0])  # Select one sibling (the first for simplicity)
                if len(selected_siblings) == n:
                    break
    
        if len(selected_siblings) < n:
            logging.error(f"Error: cannot find {n} full cores")
            sys.exit(1)
        else:
            return selected_siblings
    
    
    async def await_agent():
        start = time.time()
        agent_timeout = 60 if WEKA_PERSISTENCE_MODE != "global" else 1500  # global usually is remote storage and pre-create of logs file might take much longer
        while start + agent_timeout > time.time():
            _, _, ec = await run_command("weka local ps")
            if ec == 0:
                logging.info("Weka-agent started successfully")
                return
            await asyncio.sleep(0.3)
            logging.info("Waiting for weka-agent to start")
        raise Exception(f"Agent did not come up in {agent_timeout} seconds")
    
    
    processes = {}
    
    
    class Daemon:
        def __init__(self, cmd, alias):
            self.cmd = cmd
            self.alias = alias
            self.process = None
            self.task = None
    
        async def start(self):
            logging.info(f"Starting daemon {self.alias} with cmd {self.cmd}")
            self.task = asyncio.create_task(self.monitor())
            return self.task
    
        async def start_process(self):
            logging.info(f"Starting process {self.cmd} for daemon {self.alias}")
            self.process = await start_process(self.cmd, self.alias)
            logging.info(f"Started process {self.cmd} for daemon {self.alias}")
    
        async def stop(self):
            logging.info(f"Stopping daemon {self.alias}")
            if self.task:
                self.task.cancel()
                try:
                    await self.task
                except asyncio.CancelledError:
                    pass
            await self.stop_process()
    
        async def stop_process(self):
            logging.info(f"Stopping process for daemon {self.alias}")
            if self.process:
                await stop_process(self.process)
                self.process = None
                logging.info(f"Stopped process for daemon {self.alias}")
            logging.info(f"No process found to stop")
    
        async def monitor(self):
            async def with_pause():
                await asyncio.sleep(3)
    
            while True:
                if self.process:
                    if self.is_running():
                        await with_pause()
                        continue
                    else:
                        logging.info(f"Daemon {self.alias} is not running")
                        await self.stop_process()
                await self.start_process()
    
        def is_running(self):
            if self.process is None:
                return False
            running = self.process.returncode is None
            return running
    
    
    async def start_process(command, alias=""):
        """Start a daemon process."""
        # TODO: Check if already exists, not really needed unless actually adding recovery flow
        # TODO: Logs are basically thrown away into stdout . wrap agent logs as debug on logging level
        process = await asyncio.create_subprocess_shell(command, preexec_fn=os.setpgrp)
        # stdout=asyncio.subprocess.PIPE,
        # stderr=asyncio.subprocess.PIPE)
        logging.info(f"Daemon {alias or command} started with PID {process.pid}")
        processes[alias or command] = process
        logging.info(f"Daemon started with PID {process.pid} for command {command}")
        return process
    
    
    async def run_command(command, capture_stdout=True, log_execution=True, env: dict = None, log_output=True):
        # TODO: Wrap stdout of commands via INFO via logging
        if log_execution:
            logging.info("Running command: " + command)
        if capture_stdout:
            pipe = asyncio.subprocess.PIPE
        else:
            pipe = None
        process = await asyncio.create_subprocess_shell("set -e\n" + command,
                                                        stdout=pipe,
                                                        stderr=pipe, env=env)
        stdout, stderr = await process.communicate()
        if log_execution:
            logging.info(f"Command {command} finished with code {process.returncode}")
        if stdout and log_output:
            logging.info(f"Command {command} stdout: {stdout.decode('utf-8')}")
        if stderr and log_output:
            logging.info(f"Command {command} stderr: {stderr.decode('utf-8')}")
        return stdout, stderr, process.returncode
    
    
    async def run_logrotate():
        stdout, stderr, ec = await run_command("logrotate /etc/logrotate.conf", log_execution=False)
        if ec != 0:
            raise Exception(f"Failed to run logrotate: {stderr}")
    
    
    async def write_logrotate_config():
        with open("/etc/logrotate.conf", "w") as f:
            f.write(dedent("""
                /var/log/syslog /var/log/errors {
                    size 1M
                    rotate 10
                    missingok
                    notifempty
                    compress
                    delaycompress
                    postrotate
                       if [ -f /var/run/syslog-ng.pid ]; then
                          pkill -HUP $(cat /var/run/syslog-ng.pid)
                       else
                          echo "syslog-ng.pid not found, skipping reload" >&2
                       fi
                    endscript
                }
    """))
    
    
    async def periodic_logrotate():
        while not exiting:
            await write_logrotate_config()
            await run_logrotate()
            await asyncio.sleep(60)
    
    
    async def autodiscover_network_devices(subnet) -> List[str]:
        """Returns comma-separated list of network devices
        that belong to the given subnet.
        """
        cmd = f"ip route show {subnet} | awk '{{print $3}}'"
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to discover network devices: {stderr}")
        devices = stdout.decode('utf-8').strip().split("\n")
        devices = [d for d in devices if d]
    
        if not devices:
            logging.error(f"No network devices found for subnet {subnet}")
        else:
            logging.info(f"Discovered network devices for subnet {subnet}: {devices}")
        return devices
    
    
    async def resolve_dhcp_net(device):
        def subnet_mask_to_prefix_length(subnet_mask):
            # Convert subnet mask to binary representation
            binary_mask = ''.join([bin(int(octet) + 256)[3:] for octet in subnet_mask.split('.')])
            # Count the number of 1s in the binary representation
            prefix_length = binary_mask.count('1')
            return prefix_length
    
        def get_netdev_info(device):
            # Create a socket to communicate with the network interface
            s = None
            try:
                s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    
                # Get the IP address
                ip_address = socket.inet_ntoa(fcntl.ioctl(
                    s.fileno(),
                    0x8915,  # SIOCGIFADDR
                    struct.pack('256s', bytes(device[:15], 'utf-8'))
                )[20:24])
    
                # Get the netmask
                netmask = socket.inet_ntoa(fcntl.ioctl(
                    s.fileno(),
                    0x891b,  # SIOCGIFNETMASK
                    struct.pack('256s', bytes(device[:15], 'utf-8'))
                )[20:24])
                cidr = subnet_mask_to_prefix_length(netmask)
    
                # Get the MAC address
                info = fcntl.ioctl(s.fileno(), 0x8927,  # SIOCGIFHWADDR
                                   struct.pack('256s', bytes(device[:15], 'utf-8')))
                mac_address = ':'.join('%02x' % b for b in info[18:24])
            finally:
                if s:
                    s.close()
    
            return mac_address, ip_address, cidr
    
        try:
            mac_address, ip_address, cidr = get_netdev_info(device)
        except OSError:
            raise Exception(f"Failed to get network info for device {device}, no IP address found")
    
        return f"'{mac_address}/{ip_address}/{cidr}'"
    
    
    async def create_container():
        full_cores = find_full_cores(NUM_CORES)
        mode_part = ""
        if MODE == "compute":
            mode_part = "--only-compute-cores"
        elif MODE == "drive":
            mode_part = "--only-drives-cores"
        elif MODE == "client":
            mode_part = "--only-frontend-cores"
        elif MODE == "s3":
            mode_part = "--only-frontend-cores"
        elif MODE == "nfs":
            mode_part = "--only-frontend-cores"
    
        core_str = ",".join(map(str, full_cores))
        logging.info(f"Creating container with cores: {core_str}")
    
        # read join secret from if file exists /var/run/secrets/weka-operator/operator-user/password
        join_secret_cmd = ""
        join_secret_flag = ""
        if os.path.exists("/var/run/secrets/weka-operator/operator-user/join-secret"):
            join_secret_flag = "--join-secret"
            if MODE == "client":
                join_secret_flag = "--join-token"
            join_secret_cmd = "$(cat /var/run/secrets/weka-operator/operator-user/join-secret)"
    
        global NETWORK_DEVICE
        if not NETWORK_DEVICE and SUBNETS:
            subnets = SUBNETS.split(",")
            devices = []
            for subnet in subnets:
                d = await autodiscover_network_devices(subnet)
                devices.extend(d)
    
            NETWORK_DEVICE = ",".join(devices)
    
        if "aws_" in NETWORK_DEVICE:
            devices = [dev.replace("aws_", "") for dev in NETWORK_DEVICE.split(",")]
            net_str = " ".join([f"--net {d}" for d in devices]) + " --management-ips " + ",".join(MANAGEMENT_IPS)
        elif ',' in NETWORK_DEVICE:
            net_str = " ".join([f"--net {d}" for d in NETWORK_DEVICE.split(",")])
        else:
            if not NETWORK_DEVICE:
                raise Exception("NETWORK_DEVICE not set")
    
            if is_udp():
                net_str = f"--net udp"
            else:
                net_str = f"--net {NETWORK_DEVICE}"
    
        failure_domain = FAILURE_DOMAIN
    
        # NOTE: client containers are set up in restricted mode by default
        # (even if you login as administrator from a restricted client, your permissions will be limited to RegularUser level⁠⁠)
        command = dedent(f"""
            weka local setup container --name {NAME} --no-start --disable\
            --core-ids {core_str} --cores {NUM_CORES} {mode_part} \
            {net_str}  --base-port {PORT} \
            {f"{join_secret_flag} {join_secret_cmd}" if join_secret_cmd else ""} \
            {f"--join-ips {JOIN_IPS}" if JOIN_IPS else ""} \
            {f"--client" if MODE == 'client' else ""} \
            {f"--restricted" if MODE == 'client' and "4.2.7.64" not in IMAGE_NAME else ""} \
            {f"--failure-domain {failure_domain}" if failure_domain else ""}
        """)
        logging.info(f"Creating container with command: {command}")
        stdout, stderr, ec = await run_command(command)
        if ec != 0:
            raise Exception(f"Failed to create container: {stderr}")
        logging.info("Container created successfully")
    
    
    async def configure_traces():
        # {
        #   "enabled": true,
        #   "ensure_free_space_bytes": 3221225472,
        #   "freeze_period": {
        #     "end_time": "0001-01-01T00:00:00+00:00",
        #     "retention": 0,
        #     "start_time": "0001-01-01T00:00:00+00:00"
        #   },
        #   "retention_type": "DEFAULT",
        #   "version": 1
        # }
        global DUMPER_CONFIG_MODE
        data = dict(enabled=True, ensure_free_space_bytes=int(ENSURE_FREE_SPACE_GB) * 1024 * 1024 * 1024,
                    retention_bytes=int(MAX_TRACE_CAPACITY_GB) * 1024 * 1024 * 1024, retention_type="BYTES", version=1,
                    freeze_period=dict(start_time="0001-01-01T00:00:00+00:00", end_time="0001-01-01T00:00:00+00:00",
                                       retention=0))
        if MODE == 'dist':
            data['enabled'] = False
            data['retention_bytes'] = 128 * 1024 * 1024 * 1024
        data_string = json.dumps(data)
    
        if DUMPER_CONFIG_MODE in ["auto", ""]:
            DUMPER_CONFIG_MODE = "override"
    
        if DUMPER_CONFIG_MODE in ["override", "partial-override"]:
            command = dedent(f"""
                set -e
                mkdir -p /opt/weka/k8s-scripts
                echo '{data_string}' > /opt/weka/k8s-scripts/dumper_config.json.override
                weka local run --container {NAME} mv /opt/weka/k8s-scripts/dumper_config.json.override /data/reserved_space/dumper_config.json.{DUMPER_CONFIG_MODE}
                """)
        elif DUMPER_CONFIG_MODE == "cluster":
            command = f"""
            weka local run --container {NAME} rm -f /data/reserved_space/dumper_config.json.override
            """
        else:
            raise Exception(f"Invalid DUMPER_CONFIG_MODE: {DUMPER_CONFIG_MODE}")
    
        if command:
            stdout, stderr, ec = await run_command(command)
            if ec != 0:
                raise Exception(f"Failed to configure traces: {stderr}")
        logging.info("Traces configured successfully")
    
    
    async def ensure_aws_nics(num: int):
        command = dedent(f"""
            set -e
            mkdir -p /opt/weka/k8s-scripts
            weka local run --container {NAME} /weka/go-helpers/cloud-helper ensure-nics -n {num}
            """)
        stdout, stderr, ec = await run_command(command)
        if ec != 0:
            raise Exception(f"Failed to ensure NICs: {stderr}")
        logging.info("Ensured NICs successfully")
        write_results(dict(err=None, ensured=True, nics=json.loads(stdout.decode('utf-8').strip())['metadata']['vnics'][1:]))
    
    
    async def get_containers():
        current_containers, stderr, ec = await run_command("weka local ps --json")
        if ec != 0:
            raise Exception(f"Failed to list containers: {stderr}")
        current_containers = json.loads(current_containers)
        return current_containers
    
    
    async def get_weka_local_resources() -> dict:
        resources, stderr, ec = await run_command(f"weka local resources --container {NAME} --json", log_output=False)
        if ec != 0:
            raise Exception(f"Failed to get resources: {stderr}")
        return json.loads(resources)
    
    
    def should_recreate_client_container(resources: dict) -> bool:
        if resources["base_port"] != PORT:
            return True
        if resources.get("restricted_client") is not True:
            return True
        return False
    
    
    def convert_to_bytes(memory: str) -> int:
        size_str = memory.upper()
        match = re.match(r"(\d+)([KMGTPE]I?B)", size_str)
        if not match:
            raise ValueError(f"Invalid size format: {size_str}")
    
        size = int(match.group(1))
        unit = match.group(2)
    
        multipliers = {
            'B': 1,
            'KB': 10 ** 3,
            'MB': 10 ** 6,
            'GB': 10 ** 9,
            'TB': 10 ** 12,
            'PB': 10 ** 15,
            'EB': 10 ** 18,
            'KIB': 2 ** 10,
            'MIB': 2 ** 20,
            'GIB': 2 ** 30,
            'TIB': 2 ** 40,
            'PIB': 2 ** 50,
            'EIB': 2 ** 60
        }
        return size * multipliers[unit]
    
    
    async def ensure_weka_container():
        current_containers = await get_containers()
    
        if len(current_containers) == 0:
            logging.info("no pre-existing containers, creating")
            # create container
            if MODE in ["compute", "drive", "client", "s3", "nfs"]:
                await create_container()
            else:
                raise NotImplementedError(f"Unsupported mode: {MODE}")
    
        full_cores = find_full_cores(NUM_CORES)
    
        # reconfigure containers
        logging.info("Container already exists, reconfiguring")
        resources = await get_weka_local_resources()
    
        if MODE == "client" and should_recreate_client_container(resources):
            logging.info("Recreating client container")
            await run_command("weka local stop --force", capture_stdout=False)
            await run_command(f"weka local rm --all --force", capture_stdout=False)
            await create_container()
            resources = await get_weka_local_resources()
    
        # TODO: Normalize to have common logic between setup and reconfigure, including between clients and backends
        if MODE == "client" and len(resources['nodes']) != (NUM_CORES + 1):
            stdout, stderr, ec = await run_command(
                f"weka local resources cores -C {NAME} --only-frontend-cores {NUM_CORES} --core-ids {','.join(map(str, full_cores[:NUM_CORES]))}")
            if ec != 0:
                raise Exception(f"Failed to get frontend cores: {stderr}")
    
        # TODO: unite with above block as single getter
        resources = await get_weka_local_resources()
    
        if MODE in ["s3", "nfs"]:
            resources['allow_protocols'] = True
        resources['reserve_1g_hugepages'] = False
        resources['excluded_drivers'] = ["igb_uio"]
        resources['memory'] = convert_to_bytes(MEMORY)
        resources['auto_discovery_enabled'] = False
        resources["ips"] = MANAGEMENT_IPS
    
        # resources["mask_interrupts"] = True
    
        resources['auto_remove_timeout'] = AUTO_REMOVE_TIMEOUT
    
        cores_cursor = 0
        for node_id, node in resources['nodes'].items():
            if "MANAGEMENT" in node['roles']:
                continue
            if CPU_POLICY == "shared":
                node['dedicate_core'] = False
                node['dedicated_mode'] = "NONE"
            node['core_id'] = full_cores[cores_cursor]
            cores_cursor += 1
    
        # fix/add gateway
        if NET_GATEWAY:
            if not is_udp():
                # TODO: Multi-nic support with custom gateways
                # figure out what is meant here ^
                if len(resources['net_devices']) != 1:
                    logging.error("Gateway configuration is not supported with multiple or zero NICs")
                resources['net_devices'][0]['gateway'] = NET_GATEWAY
    
        # save resources
        with open("/tmp/weka-resources.json", "w") as f:
            json.dump(resources, f)
        # reconfigure containers
        stdout, stderr, ec = await run_command(f"""
            mv /tmp/weka-resources.json /opt/weka/data/{NAME}/container/resources-operator.json
            ln -sf resources-operator.json /opt/weka/data/{NAME}/container/resources.json
            ln -sf resources-operator.json /opt/weka/data/{NAME}/container/resources.json.stable
            ln -sf resources-operator.json /opt/weka/data/{NAME}/container/resources.json.staging
        """)
    
        # cli-based changes
        cli_changes = False
        if 'aws_' not in NETWORK_DEVICE and not is_udp():
            target_devices = set(NETWORK_DEVICE.split(","))
            if SUBNETS:
                target_devices = set(await get_devices_by_subnets(SUBNETS))
            current_devices = set(dev['device'] for dev in resources['net_devices'])
            to_remove = current_devices - target_devices
            to_add = target_devices - current_devices
            for device in to_remove:
                stdout, stderr, ec = await run_command(f"weka local resources net -C {NAME} remove {device}")
                if ec != 0:
                    raise Exception(f"Failed to remove net device {device}: {stderr}")
            for device in to_add:
                stdout, stderr, ec = await run_command(f"weka local resources net -C {NAME} add {device}")
                if ec != 0:
                    raise Exception(f"Failed to add net device {device}: {stderr}")
            cli_changes = cli_changes or len(target_devices.difference(current_devices))
    
        # applying cli-based changes
        if cli_changes:
            stdout, stderr, ec = await run_command(f"""
                ln -sf `readlink /opt/weka/data/{NAME}/container/resources.json.staging` /opt/weka/data/{NAME}/container/resources.json.stable
                ln -sf `readlink /opt/weka/data/{NAME}/container/resources.json.staging` /opt/weka/data/{NAME}/container/resources.json
            """)
    
    
        if ec != 0:
            raise Exception(f"Failed to import resources: {stderr} \n {stdout}")
    
    
    def get_boot_id():
        with open("/proc/sys/kernel/random/boot_id", "r") as file:
            boot_id = file.read().strip()
        return boot_id
    
    
    def get_instructions_dir():
        return f"/host-binds/shared/instructions/{POD_ID}/{get_boot_id()}"
    
    
    @dataclass
    class ShutdownInstructions:
        allow_force_stop: bool = False
        allow_stop: bool = False
    
    
    async def get_shutdown_instructions() -> ShutdownInstructions:
        if not POD_ID:  ## back compat mode for when pod was scheduled without downward api
            return ShutdownInstructions()
        instructions_dir = get_instructions_dir()
        instructions_file = os.path.join(instructions_dir, "shutdown_instructions.json")
    
        if not os.path.exists(instructions_file):
            ret = ShutdownInstructions()
        else:
            with open(instructions_file, "r") as file:
                data = json.load(file)
                ret = ShutdownInstructions(**data)
    
    
        if exists("/tmp/.allow-force-stop"):
            ret.allow_force_stop = True
        if exists("/tmp/.allow-stop"):
            ret.allow_stop = True
        return ret
    
    
    async def start_weka_container():
        stdout, stderr, ec = await run_command("weka local start")
        if ec != 0:
            raise Exception(f"Failed to start container: {stderr}")
        logging.info("finished applying new config")
        logging.info(f"Container reconfigured successfully: {stdout.decode('utf-8')}")
    
    
    async def configure_persistency():
        if not os.path.exists("/host-binds/opt-weka"):
            return
    
        command = dedent(f"""
            mkdir -p /opt/weka-preinstalled
            # --- save weka image data separately
            mount -o bind /opt/weka /opt/weka-preinstalled
            # --- WEKA_PERSISTENCE_DIR - is HostPath (persistent volume)
            # --- put existing drivers from persistent dir to weka-preinstalled
            mkdir -p {WEKA_PERSISTENCE_DIR}/dist/drivers
            mount -o bind {WEKA_PERSISTENCE_DIR}/dist/drivers /opt/weka-preinstalled/dist/drivers
            mount -o bind {WEKA_PERSISTENCE_DIR} /opt/weka
            mkdir -p /opt/weka/dist
            # --- put weka dist back on top
            mount -o bind /opt/weka-preinstalled/dist /opt/weka/dist
            # --- make drivers dir persistent
            mount -o bind {WEKA_PERSISTENCE_DIR}/dist/drivers /opt/weka/dist/drivers
    
            if [ -d /host-binds/boot-level ]; then
                BOOT_DIR=/host-binds/boot-level/$(cat /proc/sys/kernel/random/boot_id)/cleanup
                mkdir -p $BOOT_DIR
                mkdir -p /opt/weka/external-mounts/cleanup
                mount -o bind $BOOT_DIR /opt/weka/external-mounts/cleanup   
            fi
            
            if [ -d /host-binds/shared ]; then
                mkdir -p /host-binds/shared/local-sockets
                mkdir -p /opt/weka/external-mounts/local-sockets
                mount -o bind /host-binds/shared/local-sockets /opt/weka/external-mounts/local-sockets
            fi
            
            if [ -f /var/run/secrets/weka-operator/wekahome-cacert/cert.pem ]; then
                rm -rf /opt/weka/k8s-runtime/vars/wh-cacert
                mkdir -p /opt/weka/k8s-runtime/vars/wh-cacert/
                cp /var/run/secrets/weka-operator/wekahome-cacert/cert.pem /opt/weka/k8s-runtime/vars/wh-cacert/cert.pem
                chmod 400 /opt/weka/k8s-runtime/vars/wh-cacert/cert.pem
            fi
            
            if [ -d /host-binds/shared-configs ]; then
                ENVOY_DIR=/opt/weka/envoy
                EXT_ENVOY_DIR=/host-binds/shared-configs/envoy
                mkdir -p $ENVOY_DIR
                mkdir -p $EXT_ENVOY_DIR
                mount -o bind $EXT_ENVOY_DIR $ENVOY_DIR
            fi
            
            mkdir -p {WEKA_K8S_RUNTIME_DIR}
            touch {PERSISTENCY_CONFIGURED}
        """)
    
        stdout, stderr, ec = await run_command(command)
        if ec != 0:
            raise Exception(f"Failed to configure persistency: {stdout} {stderr}")
    
        logging.info("Persistency configured successfully")
    
    
    async def ensure_weka_version():
        cmd = "weka version | grep '*' || weka version set $(weka version)"
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to set weka version: {stderr}")
        logging.info("Weka version set successfully")
    
    
    async def configure_agent(agent_handle_drivers=False):
        logging.info(f"reconfiguring agent with handle_drivers={agent_handle_drivers}")
        ignore_driver_flag = "false" if agent_handle_drivers else "true"
    
        env_vars = dict()
    
        skip_envoy_setup = ""
        if MODE == "s3":
            skip_envoy_setup = "sed -i 's/skip_envoy_setup=.*/skip_envoy_setup=true/g' /etc/wekaio/service.conf || true"
    
        if MODE == "envoy":
            env_vars['RESTART_EPOCH_WANTED'] = str(int(os.environ.get("envoy_restart_epoch", time.time())))
            env_vars['BASE_ID'] = PORT
    
        expand_condition_mounts = ""
        if MODE in ['envoy', 's3']:
            expand_condition_mounts = ",envoy-data"
    
        drivers_handling_cmd = f"""
        # Check if the last line contains the pattern
        CONFFILE="/etc/wekaio/service.conf"
        PATTERN="skip_driver_install"
        if tail -n 1 "$CONFFILE" | grep -q "$PATTERN"; then
            sed -i '$d' "$CONFFILE"
        fi
    
    
        #TODO: once moving to 4.3+ only switch to ignore_driver_spec. Problem that 4.2 had it in different category
        # and check by skip_driver_install is sort of abuse of not anymore existing flag to have something to validate by
        if ! grep -q "skip_driver_install" /etc/wekaio/service.conf; then
            sed -i "/\[os\]/a skip_driver_install={ignore_driver_flag}" /etc/wekaio/service.conf
            sed -i "/\[os\]/a ignore_driver_spec={ignore_driver_flag}" /etc/wekaio/service.conf
        else
            sed -i "s/skip_driver_install=.*/skip_driver_install={ignore_driver_flag}/g" /etc/wekaio/service.conf
        fi
        sed -i "s/ignore_driver_spec=.*/ignore_driver_spec={ignore_driver_flag}/g" /etc/wekaio/service.conf || true
    
        sed -i "s@external_mounts=.*@external_mounts=/opt/weka/external-mounts@g" /etc/wekaio/service.conf || true
        sed -i "s@conditional_mounts_ids=.*@conditional_mounts_ids=etc-hosts,etc-resolv{expand_condition_mounts}@g" /etc/wekaio/service.conf || true
        {skip_envoy_setup}
        """
    
        cmd = dedent(f"""
            {drivers_handling_cmd}
            sed -i 's/cgroups_mode=auto/cgroups_mode=none/g' /etc/wekaio/service.conf || true
            sed -i 's/override_core_pattern=true/override_core_pattern=false/g' /etc/wekaio/service.conf || true
            sed -i "s/port=14100/port={AGENT_PORT}/g" /etc/wekaio/service.conf || true
            # sed -i "s/serve_static=false/serve_static=true/g" /etc/wekaio/service.conf || true
            echo '{{"agent": {{"port": \'{AGENT_PORT}\'}}}}' > /etc/wekaio/service.json
        """)
        stdout, stderr, ec = await run_command(cmd, env=env_vars)
        if ec != 0:
            raise Exception(f"Failed to configure agent: {stderr}")
    
        if MACHINE_IDENTIFIER is not None:
            logging.info(f"Setting machine-id {MACHINE_IDENTIFIER}")
            os.makedirs("/opt/weka/data/agent", exist_ok=True)
            cmd = f"echo '{MACHINE_IDENTIFIER}' > /opt/weka/data/agent/machine-identifier"
            stdout, stderr, ec = await run_command(cmd)
            if ec != 0:
                raise Exception(f"Failed to set machine-id: {stderr}")
        logging.info("Agent configured successfully")
    
    
    async def override_dependencies_flag():
        """Hard-code the success marker so that the dist container can start
    
        Equivalent to:
            ```sh
            HARDCODED=1.0.0-024f0fdaa33ec66087bc6c5631b85819
            mkdir -p /opt/weka/data/dependencies/HARDCODED/$(uname -r)/
            touch /opt/weka/data/dependencies/HARDCODED/$(uname -r)/successful
            ```
        """
        logging.info("overriding dependencies flag")
        dep_version = version_params.get('dependencies', DEFAULT_DEPENDENCY_VERSION)
    
        if WEKA_DRIVERS_HANDLING:
            cmd = dedent("""
            mkdir -p /opt/weka/data/dependencies
            touch /opt/weka/data/dependencies/skip
            """)
        else:
            cmd = dedent(
                f"""
                mkdir -p /opt/weka/data/dependencies/{dep_version}/$(uname -r)/
                touch /opt/weka/data/dependencies/{dep_version}/$(uname -r)/successful
                """
            )
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to override dependencies flag: {stderr}")
        logging.info("dependencies flag overridden successfully")
    
    
    async def ensure_stem_container(name="dist"):
        logging.info("ensuring dist container")
    
        cmd = dedent(f"""
            if [ -d /driver-toolkit-shared ]; then
                # Mounting kernel modules from driver-toolkit-shared to dist container
                mkdir -p /lib/modules
                mkdir -p /usr/src
                mount -o bind /driver-toolkit-shared/lib/modules /lib/modules
                mount -o bind /driver-toolkit-shared/usr/src /usr/src
            fi
    
            weka local ps | grep {name} || weka local setup container --name {name} --net udp --base-port {PORT} --no-start --disable
            """)
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to create dist container: {stderr}")
    
        logging.info("dist container created successfully")
        # wait for container to become running
    
    
    async def start_stem_container():
        logging.info("starting dist container")
        # stdout, stderr, ec = await run_command(cmd)
        # if ec != 0:
        #     raise Exception(f"Failed to start dist container: {stderr}")
        # ! start_process is deprecated and this is the only place that uses it
        # TODO: Revalidate if it needed or can be simple run_command(As it should be)
        # TODO: Still broken! hangs if running "weka local start" directly via run_command. zombie process
        await start_process(
            "weka local start")  # weka local start is not returning, so we need to daemonize it, this is a hack that needs to go away
        # reason of being stuck: agent tries to authenticate using admin:admin into this stem container, for not known reason
        logging.info("stem container started")
    
    
    async def ensure_container_exec():
        logging.info("ensuring container exec")
        start = time.time()
        while True:
            stdout, stderr, ec = await run_command(f"weka local exec --container {NAME} -- ls")
            if ec == 0:
                break
            await asyncio.sleep(1)
            if time.time() - start > 300:
                raise Exception(f"Failed to exec into container in 5 minutes: {stderr}")
        logging.info("container exec ensured")
    
    
    def write_results(results):
        logging.info("Writing result into /weka-runtime/results.json, results: \n%s", results)
        os.makedirs("/weka-runtime", exist_ok=True)
        with open("/weka-runtime/results.json.tmp", "w") as f:
            json.dump(results, f)
        os.rename("/weka-runtime/results.json.tmp", "/weka-runtime/results.json")
    
    
    async def discovery():
        # TODO: We should move here everything else we need to discover per node
        # This might be a good place to discover drives as well, as long we have some selector to discover by
        host_info = get_host_info()
        data = dict(
            is_ht=len(read_siblings_list(0)) > 1,
            kubernetes_distro=host_info.kubernetes_distro,
            os=host_info.os,
            os_build_id=host_info.os_build_id,
            schema=DISCOVERY_SCHEMA,
        )
        write_results(data)
    
    
    async def install_gsutil():
        logging.info("Installing gsutil")
        await run_command("curl https://sdk.cloud.google.com | bash -s -- --disable-prompts")
        os.environ["PATH"] += ":/root/google-cloud-sdk/bin"
        await run_command("gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS")
    
    
    async def cleanup_traces_and_stop_dumper():
        while True:
            cmd = "weka local exec supervisorctl status | grep RUNNING"
            stdout, stderr, ec = await run_command(cmd)
            if ec != 0:
                logging.info(f"Failed to get supervisorctl status: {stderr}")
                await asyncio.sleep(3)
                continue
            break
    
        stdout, stderr, ec = await run_command("""
        weka local exec supervisorctl stop weka-trace-dumper
        rm -f /opt/weka/traces/*.shard
        """)
        if ec != 0:
            logging.error(f"Failed to cleanup traces: {stderr}")
    
    
    def get_agent_cmd():
        return f"exec /usr/bin/weka --agent --socket-name weka_agent_ud_socket_{AGENT_PORT}"
    
    
    daemons = {
    
    }
    
    
    # k8s lifecycle/local leadership election
    
    
    def cos_reboot_machine():
        logging.warning("Rebooting the host")
        os.sync()
        time.sleep(3)  # give some time to log the message and sync
        os.system("echo b > /hostside/proc/sysrq-trigger")
    
    
    async def is_secure_boot_enabled():
        stdout, stderr, ec = await run_command("dmesg")
        return "Secure boot enabled" in stdout.decode('utf-8')
    
    
    async def cos_disable_driver_signing_verification():
        logging.info("Checking if driver signing is disabled")
        esp_partition = "/dev/disk/by-partlabel/EFI-SYSTEM"
        mount_path = "/tmp/esp"
        grub_cfg = "efi/boot/grub.cfg"
        sed_cmds = []
        reboot_required = False
    
        with open("/hostside/proc/cmdline", 'r') as file:
            for line in file.readlines():
                logging.info(f"cmdline: {line}")
                if "module.sig_enforce" in line:
                    if "module.sig_enforce=1" in line:
                        sed_cmds.append(('module.sig_enforce=1', 'module.sig_enforce=0'))
                else:
                    sed_cmds.append(('cros_efi', 'cros_efi module.sig_enforce=0'))
                if "loadpin.enabled" in line:
                    if "loadpin.enabled=1" in line:
                        sed_cmds.append(('loadpin.enabled=1', 'loadpin.enabled=0'))
                else:
                    sed_cmds.append(('cros_efi', 'cros_efi loadpin.enabled=0'))
                if "loadpin.enforce" in line:
                    if "loadpin.enforce=1" in line:
                        sed_cmds.append(('loadpin.enforce=1', 'loadpin.enforce=0'))
                else:
                    sed_cmds.append(('cros_efi', 'cros_efi loadpin.enforce=0'))
        if sed_cmds:
            logging.warning("Must modify kernel parameters")
            if WEKA_COS_ALLOW_DISABLE_DRIVER_SIGNING:
                logging.warning("Node driver signing configuration has changed, NODE WILL REBOOT NOW!")
            else:
                raise Exception(
                    "Node driver signing configuration must be changed, but WEKA_COS_ALLOW_DISABLE_DRIVER_SIGNING is not set to True. Exiting.")
    
            await run_command(f"mkdir -p {mount_path}")
            await run_command(f"mount {esp_partition} {mount_path}")
            current_path = os.curdir
            try:
                os.chdir(mount_path)
                for sed_cmd in sed_cmds:
                    await run_command(f"sed -i 's/{sed_cmd[0]}/{sed_cmd[1]}/g' {grub_cfg}")
                reboot_required = True
            except Exception as e:
                logging.error(f"Failed to modify kernel cmdline: {e}")
                raise
            finally:
                os.chdir(current_path)
                await run_command(f"umount {mount_path}")
                if reboot_required:
                    cos_reboot_machine()
        else:
            logging.info("Driver signing is already disabled")
    
    
    async def cos_configure_hugepages():
        if not is_google_cos():
            logging.debug("Skipping hugepages configuration")
            return
    
        with open("/proc/meminfo", 'r') as meminfo:
            for line in meminfo.readlines():
                if "HugePages_Total" in line:
                    hugepage_count = int(line.split()[1])
                    if hugepage_count > 0:
                        logging.info(f"Node already has {hugepage_count} hugepages configured, skipping")
                        return
    
        logging.info("Checking if hugepages are set")
        esp_partition = "/dev/disk/by-partlabel/EFI-SYSTEM"
        mount_path = "/tmp/esp"
        grub_cfg = "efi/boot/grub.cfg"
        sed_cmds = []
        reboot_required = False
    
        current_path = os.curdir
        with open("/hostside/proc/cmdline", 'r') as file:
            for line in file.readlines():
                logging.info(f"cmdline: {line}")
                if "hugepagesz=" in line:
                    if "hugepagesz=1g" in line.lower() and WEKA_COS_GLOBAL_HUGEPAGE_SIZE == "2m":
                        sed_cmds.append(('hugepagesz=1g', 'hugepagesz=2m'))
                    elif "hugepagesz=2m" in line.lower() and WEKA_COS_GLOBAL_HUGEPAGE_SIZE == "1g":
                        sed_cmds.append(('hugepagesz=2m', 'hugepagesz=1g'))
                if "hugepages=" not in line:
                    # hugepages= is not set at all
                    sed_cmds.append(('cros_efi', f'cros_efi hugepages={WEKA_COS_GLOBAL_HUGEPAGE_COUNT}'))
                elif f"hugepages={WEKA_COS_GLOBAL_HUGEPAGE_COUNT}" not in line and WEKA_COS_ALLOW_HUGEPAGE_CONFIG:
                    # hugepages= is set but not to the desired value, and we are allowed to change it
                    sed_cmds.append(('hugepages=[0-9]+', f'hugepages={WEKA_COS_GLOBAL_HUGEPAGE_COUNT}'))
                elif f"hugepages={WEKA_COS_GLOBAL_HUGEPAGE_COUNT}" not in line and not WEKA_COS_ALLOW_HUGEPAGE_CONFIG:
                    logging.info(f"Node hugepages configuration is managed externally, skipping")
        if sed_cmds:
            logging.warning("Must modify kernel HUGEPAGES parameters")
            if WEKA_COS_ALLOW_HUGEPAGE_CONFIG:
                logging.warning("Node hugepage configuration has changed, NODE WILL REBOOT NOW!")
            else:
                raise Exception(
                    "Node hugepage configuration must be changed, but WEKA_COS_ALLOW_HUGEPAGE_CONFIG is not set to True. Exiting.")
    
            await run_command(f"mkdir -p {mount_path}")
            await run_command(f"mount {esp_partition} {mount_path}")
            try:
                os.chdir(mount_path)
                for sed_cmd in sed_cmds:
                    await run_command(f"sed -i 's/{sed_cmd[0]}/{sed_cmd[1]}/g' {grub_cfg}")
                reboot_required = True
            except Exception as e:
                logging.error(f"Failed to modify kernel cmdline: {e}")
                raise
            finally:
                os.chdir(current_path)
                os.sync()
                await run_command(f"umount {mount_path}")
                if reboot_required:
                    cos_reboot_machine()
        else:
            logging.info(f"Hugepages are already configured to {WEKA_COS_GLOBAL_HUGEPAGE_COUNT}x2m pages")
    
    
    async def disable_driver_signing():
        if not is_google_cos():
            return
        logging.info("Ensuring driver signing is disabled")
        await cos_disable_driver_signing_verification()
    
    
    SOCKET_NAME = '\0weka_runtime_' + NAME  # Abstract namespace socket
    WEKA_K8S_RUNTIME_DIR = '/opt/weka/k8s-runtime'
    GENERATION_PATH = f'{WEKA_K8S_RUNTIME_DIR}/runtime-generation'
    CURRENT_GENERATION = str(time.time())
    PERSISTENCY_CONFIGURED = f'{WEKA_K8S_RUNTIME_DIR}/persistency-configured'
    
    
    def is_udp():
        return NETWORK_DEVICE.lower() == "udp" or UDP_MODE
    
    
    async def write_generation():
        while os.path.exists("/host-binds/opt-weka") and not os.path.exists(PERSISTENCY_CONFIGURED):
            logging.info("Waiting for persistency to be configured")
            await asyncio.sleep(1)
    
        logging.info("Writing generation %s", CURRENT_GENERATION)
        os.makedirs(WEKA_K8S_RUNTIME_DIR, exist_ok=True)
        with open(GENERATION_PATH, 'w') as f:
            f.write(CURRENT_GENERATION)
        logging.info("current generation: %s", read_generation())
    
    
    def read_generation():
        try:
            with open(GENERATION_PATH, 'r') as f:
                ret = f.read().strip()
        except Exception as e:
            logging.debug("Failed to read generation: %s", e)
            ret = ""
        return ret
    
    
    async def obtain_lock():
        server = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
        server.setblocking(False)
        server.bind(SOCKET_NAME)
        return server
    
    
    _server = None
    
    
    async def ensure_envoy_container():
        logging.info("ensuring envoy container")
        cmd = dedent(f"""
            weka local ps | grep envoy || weka local setup envoy
        """)
        _, _, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to ensure envoy container")
        pass
    
    
    def write_file(path, content):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w') as f:
            f.write(content)
    
    
    async def is_port_free(port: int) -> bool:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(('localhost', port))
                return True
            except OSError as e:
                if e.errno == 98:  # Address already in use
                    logging.debug(f"Port {port} is already in use")
                    return False
    
                logging.error(f"Failed to bind to port {port}: {e}")
                return False
    
    
    async def get_free_subrange_in_port_range(
            base_port: int,
            max_port: int,
            subrange_size: int,
            exclude_ports: Optional[List[int]] = None
    ) -> Tuple[int, int]:
        """Get a subrange of free ports of size subrange_size in the specified port range."""
        exclude_ports = sorted(exclude_ports or [])
        free_ports = set()
        not_free_ports = set()
    
        port = base_port
        while port <= max_port - subrange_size:
            # Skip any subranges that intersect with exclude_ports
            for exclude_port in exclude_ports:
                if port <= exclude_port < port + subrange_size:
                    port = exclude_port + 1
                    break
            else:
                subrange_start = port
                consecutive_free_count = 0
    
                for check_port in range(port, port + subrange_size):
                    if check_port in free_ports:
                        consecutive_free_count += 1
                    elif check_port in not_free_ports:
                        break
                    else:
                        if await is_port_free(check_port):
                            free_ports.add(check_port)
                            consecutive_free_count += 1
                        else:
                            not_free_ports.add(check_port)
                            break
    
                if consecutive_free_count == subrange_size:
                    logging.info(f"Found free subrange: {subrange_start}-{subrange_start + subrange_size - 1}")
                    return subrange_start, subrange_start + subrange_size - 1
    
                # If not all ports in the subrange were free, move to the next port
                port += 1
    
        raise RuntimeError(f"Could not find a subrange of {subrange_size} free ports in the specified range.")
    
    
    async def get_free_port(base_port: int, max_port: int, exclude_ports: Optional[List[int]] = None) -> int:
        for port in range(base_port, max_port):
            if exclude_ports and port in exclude_ports:
                continue
    
            if await is_port_free(port):
                logging.info(f"Found free port: {port}")
                return port
    
        raise RuntimeError(f"Failed to find free port in range {base_port}-{max_port}")
    
    
    async def ensure_client_ports():
        global PORT, AGENT_PORT
        logging.info("Ensuring client ports")
    
        if parse_port(PORT) > 0 and parse_port(AGENT_PORT) > 0:  # we got resources via env, so no need to wait here
            await save_weka_ports_data()
            return
    
        base_port = parse_port(BASE_PORT)
        port_range = parse_port(PORT_RANGE)
        assert base_port > 0, "BASE_PORT is not set"
        max_port = base_port + port_range if port_range > 0 else MAX_PORT
    
        try:
            if not parse_port(AGENT_PORT):
                p = await get_free_port(base_port, max_port)
                AGENT_PORT = f'{p}'
            if not parse_port(PORT):
                p1, _ = await get_free_subrange_in_port_range(base_port, max_port, WEKA_CONTAINER_PORT_SUBRANGE,
                                                              exclude_ports=[int(AGENT_PORT)])
                PORT = f'{p1}'
        except RuntimeError as e:
            raise Exception(f"Failed to find free ports: {e}")
        else:
            await save_weka_ports_data()
    
    
    async def save_weka_ports_data():
        write_file("/opt/weka/k8s-runtime/vars/port", str(PORT))
        write_file("/opt/weka/k8s-runtime/vars/agent_port", str(AGENT_PORT))
        logging.info(f"PORT={PORT}, AGENT_PORT={AGENT_PORT}")
    
    
    def parse_port(port_str: str) -> int:
        try:
            return int(port_str)
        except ValueError:
            return 0
    
    
    async def get_requested_drives():
        if not os.path.exists("/opt/weka/k8s-runtime/resources.json"):
            return []
        with open("/opt/weka/k8s-runtime/resources.json", "r") as f:
            data = json.load(f)
        return data.get("drives", [])
    
    
    async def wait_for_resources():
        global PORT, AGENT_PORT, RESOURCES, FAILURE_DOMAIN, NETWORK_DEVICE
    
        if MODE == 'client':
            await ensure_client_ports()
    
        if MODE not in ['drive', 's3', 'compute', 'nfs', 'envoy', 'client']:
            return
    
        logging.info("waiting for controller to set resources")
    
        while not os.path.exists("/opt/weka/k8s-runtime/resources.json"):
            logging.info("waiting for /opt/weka/k8s-runtime/resources.json")
            await asyncio.sleep(3)
            if (await get_shutdown_instructions()).allow_stop:
                raise Exception("Shutdown requested")
            continue
    
        with open("/opt/weka/k8s-runtime/resources.json", "r") as f:
            data = json.load(f)
    
        logging.info("found resources.json: %s", data)
        net_devices = ",".join(data.get("netDevices",[]))
        if net_devices and "aws_" in net_devices:
            NETWORK_DEVICE = net_devices
    
        if data.get("machineIdentifier"):
            logging.info("found machineIdentifier override, applying")
            global MACHINE_IDENTIFIER
            MACHINE_IDENTIFIER = data.get("machineIdentifier")
        if MODE == "client":
            return
    
        RESOURCES = data
        if "failureDomain" in data:
            FAILURE_DOMAIN = data["failureDomain"]
            logging.info("Failure Domain: %s", FAILURE_DOMAIN)
        if parse_port(PORT) == 0 and MODE != 'envoy':
            PORT = data["wekaPort"]
        if parse_port(AGENT_PORT) == 0:
            AGENT_PORT = data["agentPort"]
    
        await save_weka_ports_data()
    
    
    async def get_single_device_ip(device_name: str = "default") -> str:
        if device_name == "default":
            if IS_IPV6:
                cmd = "ip -6 addr show $(ip -6 route show default | awk '{print $5}' | head -n1) | grep 'inet6 ' | grep global | awk '{print $2}' | cut -d/ -f1"
            else:
                cmd = "ip route show default | grep src | awk '/default/ {print $9}' | head -n1"
        else:
            if IS_IPV6:
                # use ULA/GUA address for ipv6 (WEKA does not support link-local addresses)
                cmd = f"ip -6 addr show dev {device_name} | grep -E 'inet6 (fd|2)' | head -n1 | awk '{{print $2}}' | cut -d/ -f1"
            else:
                cmd = f"ip addr show dev {device_name} | grep 'inet ' | awk '{{print $2}}' | cut -d/ -f1"
    
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to get ip address for device {device_name}: {stderr}")
        ip = stdout.decode('utf-8').strip()
    
        # try again with a different command for default
        if not ip and device_name == "default":
            # TODO: support ipv6 in this case
            if not IS_IPV6:
                cmd = "ip -4 addr show dev $(ip route show default | awk '{print $5}') | grep inet | awk '{print $2}' | cut -d/ -f1"
                stdout, stderr, ec = await run_command(cmd)
                if ec != 0:
                    raise Exception(f"Failed to get ip address for device {device_name}: {stderr}")
                ip = stdout.decode('utf-8').strip()
    
        if not ip:
            raise Exception(f"Failed to get ip address for device {device_name}")
        return ip
    
    
    async def get_devices_by_subnets(subnets):
        devices = []
        for subnet in subnets.split(","):
            for d in await autodiscover_network_devices(subnet):
                devices.append(d)
        logging.info("found devices by subnets(%s): %s", subnets, devices)
        return devices
    
    
    async def write_management_ips():
        """Auto-discover management IPs and write them to a file"""
        if MODE not in ['drive', 'compute', 's3', 'nfs', 'client']:
            return
    
        ipAddresses = []
    
        if os.environ.get("MANAGEMENT_IP") and "aws_" in NETWORK_DEVICE:
            ipAddresses.append(os.environ.get("MANAGEMENT_IP"))
        elif not NETWORK_DEVICE and SUBNETS:
            devices = await get_devices_by_subnets(SUBNETS)
            for device in devices:
                ip = await get_single_device_ip(device)
                ipAddresses.append(ip)
        # default udp mode (if network device is not set explicitly)
        elif is_udp():
            if NETWORK_DEVICE != 'udp':
                ip = await get_single_device_ip(NETWORK_DEVICE)
            else:
                ip = await get_single_device_ip()
            ipAddresses.append(ip)
        # if single nic is used
        elif ',' not in NETWORK_DEVICE:
            ip = await get_single_device_ip(NETWORK_DEVICE)
            ipAddresses.append(ip)
        # if multiple nics are used
        else:
            devices = NETWORK_DEVICE.split(",")
            for device in devices:
                ip = await get_single_device_ip(device)
                ipAddresses.append(ip)
    
        if not ipAddresses:
            raise Exception("Failed to discover management IPs")
    
        with open("/opt/weka/k8s-runtime/management_ips.tmp", "w") as f:
            f.write("\n".join(ipAddresses))
        os.rename("/opt/weka/k8s-runtime/management_ips.tmp", "/opt/weka/k8s-runtime/management_ips")
    
        logging.info(f"Management IPs: {ipAddresses}")
        global MANAGEMENT_IPS
        MANAGEMENT_IPS = ipAddresses
    
    
    async def ensure_drives():
        sys_drives = await find_weka_drives()
        requested_drives = RESOURCES.get("drives", [])
        drives_to_setup = []
        for drive in requested_drives:
            for sd in sys_drives:
                if sd["serial_id"] == drive:
                    drives_to_setup.append(sd["block_device"])
                    break
            # else:
            #     raise Exception(f"Drive {drive['serial_id']} not found")
    
        # write discovered drives into runtime dir
        os.makedirs("/opt/weka/k8s-runtime", exist_ok=True)
        with open("/opt/weka/k8s-runtime/drives.json", "w") as f:
            json.dump([d for d in sys_drives if d['serial_id'] in requested_drives], f)
        logging.info(f"sys_drives: {sys_drives}")
        logging.info(f"requested_drives: {requested_drives}")
        logging.info(f"in-kernel drives are: {drives_to_setup}")
    
    
    is_legacy_driver_command = None
    
    
    async def is_legacy_driver_cmd() -> bool:
        global is_legacy_driver_command
        if is_legacy_driver_command is not None:
            return is_legacy_driver_command
        cmd = "weka driver --help | grep pack"
        stdout, stderr, ec = await run_command(cmd)
        if ec == 0:
            logging.info("Driver pack command is available, new dist mode")
            is_legacy_driver_command = False
            return False
        logging.info("Driver pack command is not available, legacy dist mode")
        is_legacy_driver_command = True
        return True
    
    
    async def pack_drivers():
        logging.info("Packing drivers")
        cmd = "weka driver pack"
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to pack drivers: {stderr}")
        logging.info("Drivers packed successfully")
    
    
    async def run_prerun_script():
        pre_run_script = os.environ.get("PRE_RUN_SCRIPT")
        if not pre_run_script:
            return
        # decode base64
        pre_run_script = base64.b64decode(pre_run_script).decode('utf-8')
        logging.info(f"Running pre-run script: {pre_run_script}")
        # save script into tmp script file
        with open("/tmp/pre-run-script.sh", "w") as f:
            f.write(pre_run_script)
        # run script
        cmd = "bash /tmp/pre-run-script.sh"
        stdout, stderr, ec = await run_command(cmd, capture_stdout=False)
        if ec != 0:
            raise Exception(f"Failed to run pre-run script: {stderr}")
    
    
    async def umount_drivers():
        #TODO: Should support specific container id
        logging.info("Umounting driver")
        find_mounts_cmd = "nsenter --target=1 --mount -- mount -t wekafs | awk '{print $3}'"
        stdout, stderr, ec = await run_command(find_mounts_cmd)
        if ec != 0:
            logging.info(f"Failed to find weka mounts: {stderr} {stdout}")
    
        errs = []
        umounted_paths = []
    
        for mount in stdout.decode('utf-8').split("\n"):
            if not mount:
                continue
            umount_cmd = f"nsenter --target=1 --mount -- umount {mount}"
            stdout, stderr, ec = await run_command(umount_cmd)
            errs.append(stderr)
            if ec != 0:
                continue
            umounted_paths.append(mount)
    
        # after umounts without errors we should succeed to rmmod, be that true or not - attempting
        if len(errs) == 0:
            stdout, stderr, ec = await run_command("""
            if lsmod | grep wekafsio; then
                rmmod wekafsio
            fi
            """
            )
            if ec != 0:
                errs.append(stderr)
    
        logging.info("weka mounts umounted successfully")
        write_results(dict(
            error=errs,
            umounted_paths = umounted_paths,
        ))
    
    async def main():
        host_info = get_host_info()
        global OS_DISTRO, OS_BUILD_ID
        OS_DISTRO = host_info.os
        logging.info(f'OS_DISTRO={OS_DISTRO}')
    
        OS_BUILD_ID = host_info.os_build_id
    
        if not OS_BUILD_ID and is_google_cos():
            raise Exception("OS_BUILD_ID is not set")
        if is_google_cos():
            logging.info(f'OS_BUILD_ID={OS_BUILD_ID}')
    
        if MODE == "discovery":
            # self signal to exit
            logging.info("discovery mode")
            await cos_configure_hugepages()
            await discovery()
            return
    
        if MODE == "drivers-loader":
            # self signal to exit
            await override_dependencies_flag()
            # 2 minutes timeout for driver loading
            end_time = time.time() + 120
            await disable_driver_signing()
            loaded = False
            while time.time() < end_time:
                try:
                    await load_drivers()
                    write_results(dict(
                        err=None,
                        drivers_loaded=True
                    ))
                    logging.info("Drivers loaded successfully")
                    loaded = True
                    return
                except Exception as e:
                    await asyncio.sleep(5)
                    if time.time() > end_time:
                        write_results(dict(
                            err=getattr(e, 'message', repr(e)),
                            drivers_loaded=False,
                        ))
                        # return (not raise) to avoid infinite container restarts in the pod
                        return
                    logging.info("retrying drivers download... will reach timeout in %d seconds", end_time - time.time())
            if not loaded:
                raise Exception("Failed to load drivers")
            return
    
        await configure_persistency()
        await wait_for_resources()
        await write_generation()  # write own generation to kill other processes
        await write_management_ips()
        global _server
        _server = await obtain_lock()  # then waiting for lock with short timeout
    
        if MODE != "adhoc-op":  # this can be specialized container that should not have agent
            await configure_agent()
            syslog = Daemon("/usr/sbin/syslog-ng -F -f /etc/syslog-ng/syslog-ng.conf", "syslog")
            await syslog.start()
    
        await override_dependencies_flag()
        if MODE not in ["dist", "drivers-dist", "drivers-loader", "drivers-builder", "adhoc-op-with-container", "envoy",
                        "adhoc-op"]:
            await ensure_drivers()
    
        if MODE != "adhoc-op":
            agent_cmd = get_agent_cmd()
            agent = Daemon(agent_cmd, "agent")
            await agent.start()
            await await_agent()
            await ensure_weka_version()
    
        if MODE == "drivers-dist":
            # Dist is only serving, we will invoke downloads on it, probably in stand-alone ad-hoc container, but never actually build
            # if DIST_LEGACY_MODE:
            logging.info("dist-service flow")
            await ensure_stem_container("dist")
            await configure_traces()
            await start_stem_container()
            await cleanup_traces_and_stop_dumper()
            return
    
        if MODE == "adhoc-op-with-container":
            global NAME
            NAME = "adhoc"
            await ensure_stem_container(NAME)
            await configure_traces()
            await start_stem_container()
            await ensure_container_exec()
            instruction = json.loads(INSTRUCTIONS)
            logging.info(f"adhoc-op-with-container instruction: {instruction}")
            payload =  json.loads(instruction['payload'])
            if instruction.get('type') == 'ensure-nics':
                if payload.get('type') == "aws":
                    await ensure_aws_nics(payload['dataNICsNumber'])
                    return
                else:
                    raise ValueError(f"Ensure NICs instruction type not supported: {payload.get('type')}")
            else:
                raise ValueError(f"unsupported instruction: {instruction.get('type')}")
    
        if MODE == "adhoc-op":
            instruction = json.loads(INSTRUCTIONS)
            if instruction.get('type') and instruction['type'] == "discover-drives":
                await discover_drives()
            elif instruction.get('type') and instruction['type'] == 'force-resign-drives':
                logging.info(f"force-resign-drives instruction: {instruction}")
                payload = json.loads(instruction['payload'])
                device_paths = payload.get('devicePaths', [])
                device_serials = payload.get('deviceSerials', [])
                if device_paths:
                    await force_resign_drives_by_paths(device_paths)
                elif device_serials:
                    await force_resign_drives_by_serials(device_serials)
            elif instruction.get('type') and instruction['type'] == 'sign-drives':
                logging.info(f"sign-drives instruction: {instruction}")
                payload = json.loads(instruction['payload'])
                signed_drives = await sign_drives(payload)
                logging.info(f"signed_drives: {signed_drives}")
                await asyncio.sleep(3)  # a hack to give kernel a chance to update paths, as it's not instant
                await discover_drives()
            # TODO: Should we support generic command proxy? security concern?
            elif instruction.get('type') and instruction['type'] == 'umount':
                logging.info(f"umounting wekafs mounts")
                await umount_drivers()
            else:
                raise ValueError(f"Unsupported instruction: {INSTRUCTIONS}")
            return
    
        # de-facto, both drivers-builder and dist right now are doing "build and serve"
        if MODE in ["dist", "drivers-builder"]:
            DIST_LEGACY_MODE = await is_legacy_driver_cmd()
            logging.info("dist-service flow")
            if is_google_cos():
                await install_gsutil()
                await cos_build_drivers()
    
            elif DIST_LEGACY_MODE:  # default
                await agent.stop()
                await configure_agent(agent_handle_drivers=True)
                await agent.start()  # here the build happens
                await await_agent()
    
            await ensure_stem_container("dist")
            await configure_traces()
            if not DIST_LEGACY_MODE:
                # there might be a better place for preRunScript, but it is needed just for driver now
                await run_prerun_script()
                await pack_drivers()  # explicit pack of drivers if supported, which is new method, that should become default with rest of code removed eventually
            else:
                await agent.stop()
                await configure_agent(agent_handle_drivers=False)
                await agent.start()
                await await_agent()
    
            if DIST_LEGACY_MODE:
                await copy_drivers()
            await start_stem_container()
            await cleanup_traces_and_stop_dumper()
            weka_version, _, _ = await run_command("weka version current")
            write_results(
                {
                    "driver_built": True,
                    "err": "",
                    "weka_version": weka_version.decode().strip(),
                    "kernel_signature": await get_kernel_signature(weka_pack_supported=not DIST_LEGACY_MODE,
                                                                   weka_drivers_handling=WEKA_DRIVERS_HANDLING),
                    "weka_pack_not_supported": DIST_LEGACY_MODE,
                    "no_weka_drivers_handling": not WEKA_DRIVERS_HANDLING,
                })
            return
    
        if MODE == "envoy":
            await ensure_envoy_container()
            return
    
        await ensure_weka_container()
        await configure_traces()
        await start_weka_container()
        await ensure_container_exec()
        logging.info("Container is UP and running")
        if MODE == "drive":
            await ensure_drives()
    
    
    async def get_kernel_signature(weka_pack_supported=False, weka_drivers_handling=False):
        if not weka_drivers_handling:
            return ""
    
        cmd = ""
        if weka_pack_supported:
            cmd = "weka driver kernel 2>&1 | awk '{printf \"%s\", $NF}'"
        else:
            # tr -d '\0' is needed to remove null character from the end of output
            cmd = "weka driver kernel-sig 2>&1 | awk '{printf \"%s\", $NF}' | tr -d '\\0'"
    
        stdout, stderr, ec = await run_command(cmd)
        if ec != 0:
            raise Exception(f"Failed to get kernel signature: {stderr}")
    
        res = stdout.decode().strip()
        assert res, "Kernel signature not found"
        return res
    
    
    async def stop_process(process):
        logging.info(f"stopping daemon with pid {process.pid} (via process group), {process}")
    
        async def cleanup_process():
            for k, v in list(processes.items()):
                if v == process:
                    logging.info(f"removing process {k}")
                    del processes[k]
            logging.info(f"waiting for process {process.pid} to exit")
            await process.wait()
            logging.info(f"process {process.pid} exited")
    
        if process.returncode is not None:
            await cleanup_process()
            return
    
        pgid = os.getpgid(process.pid)
        logging.info(f"stopping process group {pgid}")
        os.killpg(pgid, signal.SIGTERM)
        logging.info(f"process group {pgid} stopped")
        await cleanup_process()
    
    
    def is_wrong_generation():
        if MODE in ['drivers-loader', 'discovery']:
            return False
    
        current_generation = read_generation()
        if current_generation == "":
            return False
    
        if current_generation != CURRENT_GENERATION:
            logging.error("Wrong generation detected, exiting, current:%s, read: %s", CURRENT_GENERATION, read_generation())
            return True
        return False
    
    
    async def takeover_shutdown():
        while not is_wrong_generation():
            await asyncio.sleep(1)
    
        await run_command("weka local stop --force", capture_stdout=False)
    
    
    def get_active_mounts(file_path="/proc/wekafs/interface") -> int:
        """Get the number of active mounts from the specified file.
        Return -1 if the number of active mounts cannot be determined.
        """
        try:
            with open(file_path, "r") as file:
                for line in file:
                    if line.startswith("Active mounts:"):
                        # Extract the number after "Active mounts:"
                        return int(line.split(":")[1].strip())
        except FileNotFoundError:
            logging.error(f"File '{file_path}' not found.")
        except ValueError:
            logging.error(f"Failed to parse the number of active mounts.")
        except Exception as e:
            logging.error(f"Failed to get the number of active mounts: {e}")
        return -1
    
    
    async def wait_for_shutdown_instruction():
        while True:
            shutdown_instructions = await get_shutdown_instructions()
    
            if shutdown_instructions.allow_force_stop:
                logging.info("Received 'allow-force-stop' instruction")
                return
            if shutdown_instructions.allow_stop:
                logging.info("Received 'allow-stop' instruction")
                return
    
            logging.info("Waiting for shutdown instruction...")
            await asyncio.sleep(5)
    
    
    async def watch_for_force_shutdown():
        while True:
            if (await get_shutdown_instructions()).allow_force_stop:
                logging.info("Received 'allow-force-stop' instruction")
                await run_command("weka local stop --force", capture_stdout=False)
                return
            await asyncio.sleep(5)
    
    
    async def shutdown():
        global exiting
        while not (exiting or is_wrong_generation()):
            await asyncio.sleep(1)
            continue
    
        logging.warning("Received signal, stopping all processes")
        exiting = True  # multiple entry points of shutdown, exiting is global check for various conditions
    
        if MODE not in ["drivers-loader", "discovery", "ensure-nics"]:
            if MODE in ["client", "s3", "nfs", "drive", "compute"]:
                await wait_for_shutdown_instruction()
    
            force_stop = False
            if (await get_shutdown_instructions()).allow_force_stop:
                force_stop = True
            if is_wrong_generation():
                force_stop = True
            if MODE not in ["s3", "drive", "compute", "nfs"]:
                force_stop = True
            stop_flag = "--force" if force_stop else "-g"
    
            force_shutdown_task = None
            if "--force" not in stop_flag:
                force_shutdown_task = asyncio.create_task(watch_for_force_shutdown())
    
            await run_command(f"weka local stop {stop_flag}", capture_stdout=False)
            if force_shutdown_task is not None:
                force_shutdown_task.cancel()
            logging.info("finished stopping weka container")
            sys.exit(1)
    
        if MODE == "drive":
            timeout = 60
            # print out in-kernel devices for up to 60 seconds every 0.3 seconds
            requested_drives = len(await get_requested_drives())
            for _ in range(int(timeout / 0.3)):
                drives = await find_weka_drives()
                logging.info(f"Found {len(drives)}: {drives}")
                if len(drives) == requested_drives:
                    logging.info("all drives returned to kernel")
                    break
                await asyncio.sleep(0.3)
    
        for key, process in dict(processes.items()).items():
            logging.info(f"stopping process {process.pid}, {key}")
            await stop_process(process)
            logging.info(f"process {process.pid} stopped")
    
        tasks = [t for t in asyncio.all_tasks(loop) if t is not asyncio.current_task(loop)]
        [task.cancel() for task in tasks]
    
        logging.info("All processes stopped, stopping main loop")
        loop.stop()
        logging.info("Main loop stopped")
    
    
    exiting = False
    
    
    def signal_handler(sig):
        global exiting
        logging.info(f"Received signal {sig}")
        exiting = True
    
    
    def reap_zombies():
        # agent leaves zombies behind on weka local start
        while True:
            time.sleep(1)
            try:
                # Wait for any child process, do not block
                pid, _ = os.waitpid(-1, os.WNOHANG)
                if pid == 0:  # No zombie to reap
                    continue
            except ChildProcessError:
                # No child processes
                continue
    
    
    zombie_collector = threading.Thread(target=reap_zombies, daemon=True)
    zombie_collector.start()
    
    # Setup signal handler for graceful shutdown
    loop.add_signal_handler(signal.SIGINT, partial(signal_handler, "SIGINT"))
    loop.add_signal_handler(signal.SIGTERM, partial(signal_handler, "SIGTERM"))
    
    shutdown_task = loop.create_task(shutdown())
    takeover_shutdown_task = loop.create_task(takeover_shutdown())
    
    main_loop = loop.create_task(main())
    if MODE not in ["adhoc-op"]:
        logrotate_task = loop.create_task(periodic_logrotate())
    
    try:
        try:
            loop.run_until_complete(main_loop)
            loop.run_forever()
        except RuntimeError:
            if exiting:
                logging.info("Cancelled")
            else:
                raise
    finally:
        if _server is not None:
            _server.close()
        debug_sleep = int(os.environ.get("WEKA_OPERATOR_DEBUG_SLEEP", 3))
        logging.info(f"{debug_sleep} seconds exit-sleep to allow for debugging and ensure proper sync")
        start = time.time()
        while time.time() - start < debug_sleep:
            if os.path.exists("/tmp/.cancel-debug-sleep"):
                break
            time.sleep(1)
    
  devenv.sh: |
    #!/bin/bash
    # Create kernel development environment for COS
    
    set -o errexit
    set -o pipefail
    
    ROOT_MOUNT_DIR="${ROOT_MOUNT_DIR:-/root}"
    RETRY_COUNT=${RETRY_COUNT:-5}
    
    readonly COS_CI_DOWNLOAD_GCS="gs://cos-infra-prod-artifacts-official"
    readonly TOOLCHAIN_URL_FILENAME="toolchain_path"
    readonly KERNEL_HEADERS="kernel-headers.tgz"
    readonly KERNEL_HEADERS_DIR="kernel-headers"
    readonly TOOLCHAIN_ARCHIVE_GCS="toolchain.tar.xz.gcs"
    readonly TOOLCHAIN_ENV_FILENAME="toolchain_env"
    ROOT_OS_RELEASE="${ROOT_MOUNT_DIR}/etc/os-release"
    readonly RETCODE_ERROR=1
    RELEASE_ID=""
    #
    # Individual build directory, contains kernel headers for the specific build and
    # a symlink 'toolchain' that points to the toolchain used for this particular
    # build
    #
    BUILD_DIR=""
    
    KERNEL_CONFIGS="defconfig"
    BUILD_DEBUG_PACKAGE="false"
    BUILD_HEADERS_PACKAGE="false"
    CLEAN_BEFORE_BUILD="false"
    
    BOARD=""
    BUILD_ID=""
    
    # official release, CI build, or cross-toolchain
    MODE=""
    
    CROS_TC_VERSION="2021.06.26.094653"
    # Chromium OS toolchain bucket
    CROS_TC_DOWNLOAD_GCS="gs://chromiumos-sdk/"
    # COS toolchain bucket
    COS_TC_DOWNLOAD_GCS="gs://cos-sdk/"
    
    # Can be overridden by the command-line argument
    TOOLCHAIN_ARCH="x86_64"
    KERNEL_ARCH="x86_64"
    
    # CC and CXX will be set by set_compilation_env
    CC=""
    CXX=""
    
    # Use out-of-tree build for full kernel build
    KBUILD_OUTPUT="."
    
    _log() {
      local -r prefix="$1"
      shift
      echo "[${prefix}$(date -u "+%Y-%m-%d %H:%M:%S %Z")] ""$*" >&2
    }
    
    info() {
      _log "INFO    " "$*"
    }
    
    warn() {
      _log "WARNING " "$*"
    }
    
    error() {
      _log "ERROR   " "$*"
    }
    
    #######################################
    # Choose the public GCS bucket of COS to fetch files from
    # "cos-tools", "cos-tools-eu" and "cos-tools-asia"
    # based on where the VM is running.
    # Arguments:
    #   None
    # Globals:
    #   COS_DOWNLOAD_GCS
    #######################################
    get_cos_tools_bucket() {
    	# Get the zone the VM is running in.
    	# Example output: projects/438692578867/zones/us-west2-a
    	# If not running on GCE, use "cos-tools" by default.
    	metadata_zone="$(curl -s -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/zone)" || {
    		readonly COS_DOWNLOAD_GCS="gs://cos-tools"
    		return
    	}
    	zone="$( echo $metadata_zone | rev | cut -d '/' -f 1 | rev )"
    	prefix="$( echo $zone | cut -d '-' -f 1 )"
    	case $prefix in
    		"us" | "northamerica" | "southamerica")
    			readonly COS_DOWNLOAD_GCS="gs://cos-tools"
    			;;
    		"europe")
    			readonly COS_DOWNLOAD_GCS="gs://cos-tools-eu"
    			;;
    		"asia" | "australia")
    			readonly COS_DOWNLOAD_GCS="gs://cos-tools-asia"
    			;;
    		*)
    			readonly COS_DOWNLOAD_GCS="gs://cos-tools"
    			;;
    	esac
    }
    
    download_from_url() {
      local -r url="$1"
      local -r output="$2"
      info "Downloading from URL: ${url}"
      info "Local download location: ${output}"
      local attempts=0
      until curl --http1.1 -sfS "${url}" -o "${output}"; do
        attempts=$(( attempts + 1))
        if (( "${attempts}" >= "${RETRY_COUNT}" )); then
          error "Could not download from ${url}"
          return ${RETCODE_ERROR}
        fi
        warn "Error downloading from ${url}, retrying"
        sleep 1
      done
      info "Download finished"
    }
    
    download_from_gcs() {
      local -r url="$1"
      local -r output="$2"
      info "Downloading from Google Storage: ${url}"
      info "Local download location: ${output}"
      local attempts=0
      until gsutil -q cp "${url}" "${output}"; do
        attempts=$(( attempts + 1))
        if (( "${attempts}" >= "${RETRY_COUNT}" )); then
          error "Could not download from ${url}"
          return ${RETCODE_ERROR}
        fi
        warn "Error downloading from ${url}, retrying"
        sleep 1
      done
      info "Download finished"
    }
    
    # Get the toolchain description in the form of $toolchain-$version
    # For CrOS toolchain it's just a basename without extension but for
    # COS toolchain version needs to be extarcted from the GCS bucket path
    get_toolchain_pkg_name() {
      local -r download_url=$1
      case "${download_url}" in
        *//cos-sdk/*)
          local -r toolchain="$(basename -s .tar.xz "${download_url}")"
          local -r path="$(echo "${download_url}" | sed 's@\w\+://cos-sdk/@@')"
          local -r version="$(echo "${path}" | awk -F / '{print $1 "-" $2}')"
          echo "${toolchain}-${version}"
          ;;
        *//chromiumos-sdk/*)
          echo "$(basename -s .tar.xz "${download_url}")"
          ;;
        *)
          error "Unknown toolchain source: ${download_url}"
          exit ${RETCODE_ERROR}
          ;;
      esac
    }
    
    install_cross_toolchain_pkg() {
      local -r download_url=$1
      local -r tmpdownload="$(mktemp -d)"
      local -r archive_name="$(basename "${download_url}")"
      local -r pkg_name="$(get_toolchain_pkg_name "${download_url}")"
      local -r toolchain_dir="/build/toolchains/${pkg_name}"
      if [[ ! -d "${toolchain_dir}" ]]; then
        info "Downloading prebuilt toolchain from ${download_url}"
        download_from_gcs "${download_url}" "${tmpdownload}/${archive_name}"
        # Don't unpack Rust toolchain elements because they are not needed and they
        # use a lot of disk space.
        mkdir -p "${toolchain_dir}"
        info "Unpacking toolchain to ${toolchain_dir}"
        tar axf "${tmpdownload}/${archive_name}" -C "${toolchain_dir}" \
          --exclude='./usr/lib64/rustlib*' \
          --exclude='./usr/lib64/libstd-*.so' \
          --exclude='./lib/libstd-*.so' \
          --exclude='./lib/librustc*' \
          --exclude='./usr/lib64/librustc*'
        rm -rf "${tmpdownload}"
        info "Toolchain installed"
      else
        info "Toolchain is already cached"
      fi
    
      if [[ ! -L "${BUILD_DIR}/toolchain" ]]; then
        ln -s "${toolchain_dir}" "${BUILD_DIR}/toolchain"
      fi
    
      # keep toolchain source information
      echo -n "${download_url}" > "${BUILD_DIR}/toolchain_url"
    }
    
    install_release_cross_toolchain() {
      info "Downloading and installing a toolchain"
      # Get toolchain_env path from COS GCS bucket
      local -r tc_env_file_path="${COS_DOWNLOAD_GCS}/${RELEASE_ID}/${TOOLCHAIN_ENV_FILENAME}"
      info "Obtaining toolchain_env file from ${tc_env_file_path}"
    
      # Download toolchain_env if present
      if ! download_from_gcs "${tc_env_file_path}" "${BUILD_DIR}/${TOOLCHAIN_ENV_FILENAME}"; then
        error "Failed to download toolchain file"
        error "Make sure build id '$RELEASE_ID' is valid"
        return ${RETCODE_ERROR}
      fi
    
      # Download .gcs file with the original location of the toolchain
      # we need the version to put it in cachable location
      local -r tc_gcs_download_url="${COS_DOWNLOAD_GCS}/${RELEASE_ID}/${TOOLCHAIN_ARCHIVE_GCS}"
      if ! download_from_gcs "${tc_gcs_download_url}" "${BUILD_DIR}/${TOOLCHAIN_ARCHIVE_GCS}"; then
        error "Failed to download toolchain .gcs file"
        error "Make sure build id '$RELEASE_ID' is valid"
        return ${RETCODE_ERROR}
      fi
    
      local -r bucket=$(cat "${BUILD_DIR}/${TOOLCHAIN_ARCHIVE_GCS}" | grep ^bucket: | cut -d ' ' -f 2)
      local -r path=$(cat "${BUILD_DIR}/${TOOLCHAIN_ARCHIVE_GCS}" | grep ^path: | cut -d ' ' -f 2)
      local -r tc_download_url="gs://$bucket/$path"
    
      # Install toolchain pkg
      install_cross_toolchain_pkg "${tc_download_url}"
    }
    
    install_release_kernel_headers() {
      info "Downloading and installing a kernel headers"
      local -r kernel_headers_file_path="${COS_DOWNLOAD_GCS}/${RELEASE_ID}/${KERNEL_HEADERS}"
      info "Obtaining kernel headers file from ${kernel_headers_file_path}"
    
      if ! download_from_gcs "${kernel_headers_file_path}" "${BUILD_DIR}/${KERNEL_HEADERS}"; then
            return ${RETCODE_ERROR}
      fi
      mkdir -p "${BUILD_DIR}/${KERNEL_HEADERS_DIR}"
      tar axf "${BUILD_DIR}/${KERNEL_HEADERS}" -C "${BUILD_DIR}/${KERNEL_HEADERS_DIR}"
      rm -f "${BUILD_DIR}/${KERNEL_HEADERS}"
    }
    
    # Download and install toolchain from the CI or tryjob build directory
    install_build_cross_toolchain() {
      local -r bucket="$1"
    
      info "Downloading and installing a toolchain"
      # Get toolchain_env path from COS GCS bucket
      local -r tc_env_file_path="${bucket}/${TOOLCHAIN_ENV_FILENAME}"
      local -r tc_url_file_path="${bucket}/${TOOLCHAIN_URL_FILENAME}"
    
      info "Obtaining toolchain_env file from ${tc_env_file_path}"
    
      # Download toolchain_env if present
      if ! download_from_gcs "${tc_env_file_path}" "${BUILD_DIR}/${TOOLCHAIN_ENV_FILENAME}"; then
            error "Failed to download toolchain file"
            error "Make sure build id '$RELEASE_ID' is valid"
            return ${RETCODE_ERROR}
      fi
    
      # Download toolchain_path if present
      if ! download_from_gcs "${tc_url_file_path}" "${BUILD_DIR}/${TOOLCHAIN_URL_FILENAME}"; then
            error "Failed to download toolchain file"
            error "Make sure build id '$RELEASE_ID' is valid"
            return ${RETCODE_ERROR}
      fi
    
      local -r tc_path="$(cat ${BUILD_DIR}/${TOOLCHAIN_URL_FILENAME})"
      local tc_download_url="${COS_TC_DOWNLOAD_GCS}${tc_path}"
      if ! gsutil -q stat "${tc_download_url}"; then
        tc_download_url="${CROS_TC_DOWNLOAD_GCS}${tc_path}"
      fi
    
      if ! gsutil -q stat "${tc_download_url}"; then
            error "Toolchain path '${tc_path}' does not exist in either COS or CrOS GCS buckets"
            return ${RETCODE_ERROR}
      fi
    
      # Install toolchain pkg
      install_cross_toolchain_pkg "${tc_download_url}"
    }
    
    # Download and install kernel headers from the CI or tryjob build directory
    install_build_kernel_headers() {
      local -r bucket="$1"
    
      info "Downloading and installing a kernel headers"
      local -r kernel_headers_file_path="${bucket}/${KERNEL_HEADERS}"
      info "Obtaining kernel headers file from ${kernel_headers_file_path}"
    
      if ! download_from_gcs "${kernel_headers_file_path}" "${BUILD_DIR}/${KERNEL_HEADERS}"; then
            return ${RETCODE_ERROR}
      fi
      mkdir -p "${BUILD_DIR}/${KERNEL_HEADERS_DIR}"
      tar axf "${BUILD_DIR}/${KERNEL_HEADERS}" -C "${BUILD_DIR}/${KERNEL_HEADERS_DIR}"
      rm -f "${BUILD_DIR}/${KERNEL_HEADERS}"
    }
    
    install_generic_cross_toolchain() {
      info "Downloading and installing a toolchain"
      # Download toolchain_env if present
      local -r tc_date="$(echo ${CROS_TC_VERSION} | sed  -E 's/\.(..).*/\/\1/')"
      local -r tc_download_url="${CROS_TC_DOWNLOAD_GCS}${tc_date}/${TOOLCHAIN_ARCH}-cros-linux-gnu-${CROS_TC_VERSION}.tar.xz"
    
      # Install toolchain pkg
      install_cross_toolchain_pkg "${tc_download_url}"
    }
    
    set_compilation_env() {
      local -r tc_env_file_path="${COS_DOWNLOAD_GCS}/${RELEASE_ID}/${TOOLCHAIN_ENV_FILENAME}"
      # toolchain_env file will set 'CC' and 'CXX' environment
      # variable based on the toolchain used for kernel compilation
      if [[ -f "${BUILD_DIR}/${TOOLCHAIN_ENV_FILENAME}" ]]; then
        source "${BUILD_DIR}/${TOOLCHAIN_ENV_FILENAME}"
        export CC
        export CXX
      else
        export CC="${TOOLCHAIN_ARCH}-cros-linux-gnu-clang"
        export CXX="${TOOLCHAIN_ARCH}-cros-linux-gnu-clang++"
      fi
      info "Configuring environment variables for cross-compilation"
      # CC and CXX are already set in toolchain_env
      TOOLCHAIN_DIR="${BUILD_DIR}/toolchain"
      export PATH="${TOOLCHAIN_DIR}/bin:${TOOLCHAIN_DIR}/usr/bin:${PATH}"
      export SYSROOT="${TOOLCHAIN_DIR}/usr/${TOOLCHAIN_ARCH}-cros-linux-gnu"
      export HOSTCC="x86_64-pc-linux-gnu-clang"
      export HOSTCXX="x86_64-pc-linux-gnu-clang++"
      export LD="${TOOLCHAIN_ARCH}-cros-linux-gnu-ld.lld"
      export HOSTLD="x86_64-pc-linux-gnu-ld.lld"
      export OBJCOPY=llvm-objcopy
      export STRIP=llvm-strip
      export KERNEL_ARCH
      export TOOLCHAIN_ARCH
      export LLVM_IAS=1
      if [[ "${MODE}" = "release" || "${MODE}" = "build" || "${MODE}" = "custom" ]]; then
        local -r headers_dir=$(ls -d ${BUILD_DIR}/${KERNEL_HEADERS_DIR}/usr/src/linux-headers*)
        export KHEADERS="${headers_dir}"
      fi
    }
    
    kmake() {
      local output_dir_arg="KBUILD_OUTPUT="
      if [[ "${KBUILD_OUTPUT}" != "." ]]; then
        output_dir_arg="KBUILD_OUTPUT=${KBUILD_OUTPUT}"
      fi
      env ARCH=${KERNEL_ARCH} make ARCH=${KERNEL_ARCH} \
        CC="${CC}" CXX="${CXX}" LD="${LD}" \
        STRIP="${STRIP}" OBJCOPY="${OBJCOPY}" \
        HOSTCC="${HOSTCC}" HOSTCXX="${HOSTCXX}" HOSTLD="${HOSTLD}" \
        "${output_dir_arg}" \
        "$@"
    }
    export -f kmake
    
    gpu_build() {
      if [[ ${KERNEL_ARCH} != "x86_64" ]]; then
        echo "GPU driver builds only tested for x86.
        Current architecture detected: ${KERNEL_ARCH}"
        exit 1
      fi
      make -C "/src/${GPU_DIR}" modules VERBOSE=1 V=1 \
        SYSSRC="/src/" \
        TARGET_ARCH=${KERNEL_ARCH} \
        CC="x86_64-cros-linux-gnu-clang" \
        LD="x86_64-cros-linux-gnu-ld.bfd" \
        AR="x86_64-cros-linux-gnu-ar" \
        CXX="x86_64-cros-linux-gnu-gcc" \
        OBJCOPY="x86_64-cros-linux-gnu-objcopy" \
        OBJDUMP="x86_64-cros-linux-gnu-objdump" \
        NV_VERBOSE=1 IGNORE_CC_MISMATCH=yes \
        "$@"
    }
    
    tar_kernel_headers() {
      local -r version=$(kmake "$@" -s kernelrelease)
      local -r tmpdir="$(mktemp -d)"
      local arch_dir
      case "${KERNEL_ARCH}" in
        x86_64) arch_dir="x86" ;;
        arm64)  arch_dir="arm64" ;;
        *)
          echo "Unknown kernel architecture: ${KERNEL_ARCH}"
          exit $RETCODE_ERROR
          ;;
      esac
    
      (
        find . -name Makefile\* -o -name Kconfig\* -o -name \*.pl
        find arch/*/include include scripts -type f -o -type l
        find "arch/${arch_dir}" -name module.lds -o -name Kbuild.platforms -o -name Platform
        find "arch/${arch_dir}" -name include -o -name scripts -type d | while IFS='' read -r line; do
          find "${line}" -type f
        done
      ) > "${tmpdir}/hdrsrcfiles"
    
      pushd "${KBUILD_OUTPUT}"
      (
        if [[ -d tools/objtool ]]; then
          find tools/objtool -type f -executable
        fi
        find "arch/${arch_dir}/include" Module.symvers System.map \
          include scripts .config \
          -type f ! -name "*.cmd"  ! -name "*.o"
      ) > "${tmpdir}/hdrobjfiles"
      popd
    
      local -r destdir="${tmpdir}/headers_tmp/usr/src/linux-headers-${version}"
      mkdir -p "${destdir}"
      mkdir -p "${destdir}/build"
      tar -c -f - -T "${tmpdir}/hdrsrcfiles" | tar -xf - -C "${destdir}"
      # separate generated files and main sources for now
      # this is to prevent breakage in linux-info.eclass that
      # rely on src and build being separated
      tar -c -f - -C ${KBUILD_OUTPUT} -T "${tmpdir}/hdrobjfiles" | tar -xf - -C "${destdir}/build"
      echo "include ../Makefile" > "${destdir}/build/Makefile"
    
      rm "${tmpdir}/hdrsrcfiles" "${tmpdir}/hdrobjfiles"
    
      tar -C "${tmpdir}/headers_tmp" -c -z -f "cos-kernel-headers-${version}-${KERNEL_ARCH}.tgz" .
      rm -rf "${tmpdir}"
    }
    
    kernel_build() {
      local -r tmproot_dir="$(mktemp -d)"
      local image_target
    
      case "${KERNEL_ARCH}" in
        x86_64)   image_target="bzImage" ;;
        arm64) image_target="Image" ;;
        *)
          echo "Unknown kernel architecture: ${KERNEL_ARCH}"
          exit $RETCODE_ERROR
          ;;
      esac
    
      if [[ "${CLEAN_BEFORE_BUILD}" = "true" ]]; then
        kmake "$@" mrproper
      fi
      kmake "$@" "${KERNEL_CONFIGS[@]}"
      kmake "$@" "${image_target}" modules
      # kernelrelease should be evaluated after the build
      # otherwise CONFIG_LOCALVERSION value is not picked up properly
      local -r version=$(kmake "$@" -s kernelrelease)
      INSTALL_MOD_PATH="${tmproot_dir}" kmake "$@" modules_install
    
      mkdir -p "${tmproot_dir}/boot/"
      cp -v -- "${KBUILD_OUTPUT}/.config" "${tmproot_dir}/boot/config-${version}"
      cp -v -- "${KBUILD_OUTPUT}/arch/${KERNEL_ARCH}/boot/${image_target}" "${tmproot_dir}/boot/vmlinuz-${version}"
    
      for module in $(find "$tmproot_dir/lib/modules/" -name "*.ko" -printf '%P\n'); do
        module="lib/modules/$module"
        mkdir -p "$(dirname "$tmproot_dir/usr/lib/debug/$module")"
        # only keep debug symbols in the debug file
        $OBJCOPY --only-keep-debug "$tmproot_dir/$module" "$tmproot_dir/usr/lib/debug/$module"
        # strip original module from debug symbols
        $OBJCOPY --strip-debug "$tmproot_dir/$module"
        # then add a link to those
        $OBJCOPY --add-gnu-debuglink="$tmproot_dir/usr/lib/debug/$module" "$tmproot_dir/$module"
      done
    
      if [[ "${BUILD_DEBUG_PACKAGE}" = "true" ]]; then
        cp -v -- "${KBUILD_OUTPUT}/vmlinux" "${tmproot_dir}/usr/lib/debug/lib/modules/${version}/"
        # Some other tools expect other locations
        mkdir -p "$tmproot_dir/usr/lib/debug/boot/"
        ln -s "../lib/modules/$version/vmlinux" "$tmproot_dir/usr/lib/debug/boot/vmlinux-$version"
        ln -s "lib/modules/$version/vmlinux" "$tmproot_dir/usr/lib/debug/vmlinux-$version"
        tar -c -J -f "cos-kernel-debug-${version}-${KERNEL_ARCH}.txz" -C "${tmproot_dir}/usr/lib" debug/
      fi
    
      tar -c -J -f "cos-kernel-${version}-${KERNEL_ARCH}.txz" -C "${tmproot_dir}" boot/ lib/
      rm -rf "${tmproot_dir}"
    
      if [[ "${BUILD_HEADERS_PACKAGE}" = "true" ]]; then
        tar_kernel_headers
      fi
    
      # pass env information
      echo "CC=${CC}" >  "${KBUILD_OUTPUT}/toolchain_env"
      echo "CXX=${CXX}" >>  "${KBUILD_OUTPUT}/toolchain_env"
    
      # pass toolchain source location
      if [[ -f "${BUILD_DIR}/toolchain_url" ]]; then
        cp "${BUILD_DIR}/toolchain_url" "${KBUILD_OUTPUT}/toolchain_url";
      fi
    }
    
    module_build() {
      if [[ "${CLEAN_BEFORE_BUILD}" = "true" ]]; then
        kmake -C "${KHEADERS}" M="$(pwd)" "$@" clean
      fi
      kmake -C "${KHEADERS}" M="$(pwd)" "$@" modules
    }
    
    usage() {
    cat 1>&2 <<__EOUSAGE__
    Usage: $0 [-k | -m | -i] [-cdH] [-A <x86_64|arm64>]
        [-C <kernelconfig>[,fragment1.config,...]] [-O  <objdir>]
        [-B <build> -b <board> | -R <release> | -G <bucket>]
        [-t <toolchain_version>] [VAR=value ...] [target ...]
    
    Options:
      -A <arch>     target architecture. Valid values are x86_64 and arm64.
      -B <build>    seed the toolchain from the COS build <build>.
                    Example: R93-16623.0.0. Instead of the actual
                    build number developer can specify the branch name
                    to use the latest build off that branch.
                    Example: main-R93, release-R89. Requires -b option.
      -C <configs>  kernel configs target. Example: lakitu_defconfig.
                    It's also possible to specify main config and fragments
                    separated by coma, i.e.: lakitu_defconfig,google/xfstest.config
      -G <bucket>   seed the toolchain and kernel headers from the custom
                    GCS bucket <bucket>. Directory structure needs to conform
                    to the COS standard.
      -H            create a package with kernel headers for the respective
                    kernel package. Should be used only with -k option.
      -O <objdir>   value for KBUILD_OUTPUT to separate obj files from
                    sources
      -R <release>  seed the toolchain and kernel headers from the
                    specified official COS release. Example: 16442.0.0
      -b <board>    specify board for -B argument. Example: lakitu
      -c            perform "mrproper" step when building a kernel package or
                    "clean" step when building a module.
                    Should be used only with -k and -m option.
      -d            create a package with debug symbols for the respective
                    kernel package. Should be used only with -k option.
      -h            show this message.
      -i            invoke interactive shell with kernel development
                    environment initialized.
      -k            build a kernel package for sources mapped from the host
                    to the current working directory.
      -m            build an out-of-tree module for sources mapped from
                    the host to the current working directory.
                    This mode requires either -R or -B/b options.
      -t            seed the toolchain from the Chromium OS upstream.
                    Example: 2021.06.26.094653
      -x <src>      build the nvidia gpu modules from the specified source relative
                    to the kernel source directory. Output nvidia gpu modules
                    present in the <x>/kernel-open/ dir.
                    Example: -x nvidia/kernel-module-src, modules generated in:
                    nvidia/kernel-module-src/kernel-open as nvidia*.ko
    __EOUSAGE__
    
      exit $RETCODE_ERROR
    }
    
    main() {
      local build_target=""
      local custom_bucket=""
      get_cos_tools_bucket
      while getopts "A:B:C:G:HO:R:b:cdhikmtx:" o; do
        case "${o}" in
          A) KERNEL_ARCH=${OPTARG} ;;
          B) BUILD_ID=${OPTARG} ;;
          C) KERNEL_CONFIGS=(${OPTARG//,/ }) ;;
          G) custom_bucket=${OPTARG} ;;
          H) BUILD_HEADERS_PACKAGE="true" ;;
          O) KBUILD_OUTPUT=${OPTARG} ;;
          R) RELEASE_ID=${OPTARG} ;;
          b) BOARD=${OPTARG} ;;
          c) CLEAN_BEFORE_BUILD="true" ;;
          d) BUILD_DEBUG_PACKAGE="true" ;;
          h) usage ;;
          i) build_target="shell" ;;
          k) build_target="kernel" ;;
          m) build_target="module" ;;
          t) CROS_TC_VERSION="${OPTARG}" ;;
          x) build_target="gpu"
            GPU_DIR=${OPTARG} ;;
          *) usage ;;
        esac
      done
      shift $((OPTIND-1))
    
      if [[ ! -z "${BOARD}" ]]; then
        case "${BOARD}" in
          lakitu-arm64) KERNEL_ARCH=arm64 ;;
          *) KERNEL_ARCH=x86_64 ;;
        esac
      fi
    
      case "${KERNEL_ARCH}" in
        x86_64)
          TOOLCHAIN_ARCH=x86_64
          ;;
        arm64)
          TOOLCHAIN_ARCH=aarch64
          ;;
        *)
          echo "Invalid -A value: $KERNEL_ARCH"
          usage
          ;;
      esac
    
      echo "** Kernel architecture: $KERNEL_ARCH"
      echo "** Toolchain architecture: $TOOLCHAIN_ARCH"
    
      if [[ -n "$RELEASE_ID" ]]; then
        MODE="release"
        BUILD_DIR="/build/${TOOLCHAIN_ARCH}-${RELEASE_ID}"
        echo "** COS release: $RELEASE_ID"
      fi
    
      if [[ -n "$BUILD_ID" ]]; then
        if ! [[ $BUILD_ID =~ R[0-9]+-[0-9.]+ ]]; then
          BRANCH="${BUILD_ID}"
          echo "** Obtaining the latest build # for branch ${BRANCH}..."
          readonly latest="${COS_CI_DOWNLOAD_GCS}/${BOARD}-release/LATEST-${BUILD_ID}"
          BUILD_ID=$(gsutil -q cat "${latest}" || true)
          if [[ -n "$BUILD_ID" ]]; then
            echo "** Latest build for branch ${BRANCH} is ${BUILD_ID}"
          else
            echo "** Failed to find latest build for branch ${BRANCH}"
            exit 1
          fi
        fi
      fi
    
      if [[ -z "$MODE" && -n "$BOARD" && -n "$BUILD_ID" ]]; then
        MODE="build"
        echo "** COS build: $BOARD-$BUILD_ID"
        BUILD_DIR="/build/${BOARD}-${BUILD_ID}"
      fi
    
      if [[ -z "$MODE" && -n "$custom_bucket" ]]; then
        MODE="custom"
        BUILD_DIR="/build/$(basename "${custom_bucket}")"
      fi
    
      if [[ -z "$MODE" ]]; then
        MODE="cross"
        BUILD_DIR="/build/cros-${CROS_TC_VERSION}-${TOOLCHAIN_ARCH}"
      fi
      echo "Mode: $MODE"
    
      if [[ -n "${BUILD_DIR}" ]]; then
        if [[ ! -d "${BUILD_DIR}" ]]; then
          mkdir -p "${BUILD_DIR}"
          case "$MODE" in
            cross)
              install_generic_cross_toolchain
              ;;
            release)
              install_release_cross_toolchain
              install_release_kernel_headers
              ;;
            build)
              local -r bucket="${COS_CI_DOWNLOAD_GCS}/${BOARD}-release/${BUILD_ID}"
              install_build_cross_toolchain "${bucket}"
              install_build_kernel_headers "${bucket}"
              ;;
            custom)
              install_build_cross_toolchain "${custom_bucket}"
              install_build_kernel_headers "${custom_bucket}"
              ;;
          esac
        fi
      fi
    
      set_compilation_env
    
      case "${build_target}" in
        kernel) kernel_build -j"$(nproc)" ;;
        module) module_build -j"$(nproc)" ;;
        shell)
          echo "Starting interactive shell for the kernel devenv"
          /bin/bash
          ;;
        gpu) gpu_build -j"$(nproc)" ;;
        *) kmake -j"$(nproc)" "$@" ;;
      esac
    }
    
    main "$@"
---
# Source: weka-operator/templates/auth_proxy_client_clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: metrics-reader
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/part-of: weka-operator
    app.kubernetes.io/managed-by: kustomize
  name: "weka-operator-metrics-reader"
rules:
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: weka-operator/templates/auth_proxy_role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: proxy-role
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-proxy-role"
rules:
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
# Source: weka-operator/templates/client_editor_role.yaml
# permissions for end users to edit clients.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: client-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/part-of: weka-operator
    app.kubernetes.io/managed-by: kustomize
  name: client-editor-role
rules:
- apiGroups:
  - weka.weka.io
  resources:
  - clients
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - weka.weka.io
  resources:
  - clients/status
  verbs:
  - get
---
# Source: weka-operator/templates/client_viewer_role.yaml
# permissions for end users to view clients.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: client-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/part-of: weka-operator
    app.kubernetes.io/managed-by: kustomize
  name: client-viewer-role
rules:
- apiGroups:
  - weka.weka.io
  resources:
  - clients
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - weka.weka.io
  resources:
  - clients/status
  verbs:
  - get
---
# Source: weka-operator/templates/node_describe_role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-node-describe-role"
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get"]
---
# Source: weka-operator/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: weka-operator-manager-role
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - create
  - get
  - list
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods
  - services
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - pods/exec
  verbs:
  - create
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - create
  - delete
  - get
  - list
  - update
  - watch
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - weka.weka.io
  resources:
  - driveclaims
  - wekaclients
  - wekaclusters
  - wekacontainers
  - wekamanualoperations
  - wekapolicies
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - weka.weka.io
  resources:
  - driveclaims/finalizers
  - wekaclients/finalizers
  - wekaclusters/finalizers
  - wekacontainers/finalizers
  - wekamanualoperations/finalizers
  - wekapolicies/finalizers
  verbs:
  - update
- apiGroups:
  - weka.weka.io
  resources:
  - driveclaims/status
  - wekaclients/status
  - wekaclusters/status
  - wekacontainers/status
  - wekamanualoperations/status
  - wekapolicies/status
  verbs:
  - get
  - patch
  - update
---
# Source: weka-operator/templates/auth_proxy_role_binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: proxy-rolebinding
    app.kubernetes.io/name: clusterrolebinding
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-proxy-rolebinding"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: "weka-operator-proxy-role"
subjects:
  - kind: ServiceAccount
    name: "weka-operator-controller-manager"
    namespace: "weka-operator-system"
---
# Source: weka-operator/templates/role_binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: manager-rolebinding
    app.kubernetes.io/name: clusterrolebinding
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-manager-rolebinding"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: "weka-operator-manager-role"
subjects:
  - kind: ServiceAccount
    name: "weka-operator-controller-manager"
    namespace: "weka-operator-system"
---
# Source: weka-operator/templates/leader_election_role.yaml
# permissions to do leader election.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: leader-election-role
    app.kubernetes.io/name: role
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-leader-election-role"
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: weka-operator/templates/leader_election_role_binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: leader-election-rolebinding
    app.kubernetes.io/name: rolebinding
    app.kubernetes.io/part-of: weka-operator
  name: "weka-operator-leader-election-rolebinding"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "weka-operator-leader-election-role"
subjects:
  - kind: ServiceAccount
    name: "weka-operator-controller-manager"
    namespace: "weka-operator-system"
---
# Source: weka-operator/templates/auth_proxy_service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: controller-manager-metrics-service
    app.kubernetes.io/name: service
    app.kubernetes.io/part-of: weka-operator
    control-plane: controller-manager
  name: "weka-operator-controller-manager-metrics-service"
spec:
  ports:
    - name: https
      port: 8443
      protocol: TCP
      targetPort: https
  selector:
    control-plane: controller-manager
---
# Source: weka-operator/templates/metrics_daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: "weka-operator-node-agent"
  labels:
    app.kubernetes.io/component: weka-node-agent
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: weka-node-agent
    app.kubernetes.io/name: weka-node-agent
    app.kubernetes.io/part-of: weka-operator
    control-plane: node-agent
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  selector:
    matchLabels:
      control-plane: weka-node-agent
      app.kubernetes.io/component: weka-node-agent
  template:
    metadata:
      annotations:
        "prometheus.io/scrape": "true"
        "prometheus.io/port": "8090"
        "prometheus.io/path": "/metrics"
        "kubectl.kubernetes.io/default-container": node-agent
      labels:
        control-plane: weka-node-agent
        app.kubernetes.io/created-by: weka-operator
        app.kubernetes.io/component: weka-node-agent
        app: weka-node-agent
    spec:
      securityContext:
        runAsNonRoot: false
      imagePullSecrets:
        - name: "quay-io-robot-secret"
      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
                  - key: kubernetes.io/os
                    operator: In
                    values:
                      - linux
      tolerations:
      dnsPolicy: 
      volumes:
        - name: tmpdir
          emptyDir: { }
        - name: token
          secret:
              secretName: "weka-node-agent-secret"
        - name: weka-persistence
          hostPath:
            path: /opt/k8s-weka/shared

      containers:
        - command:
            - /weka-operator
          ports:
            - containerPort: 8090
              name: http
              protocol: TCP
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: OTEL_DEPLOYMENT_IDENTIFIER
              value: ""
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "https://otelcollector.rnd.weka.io:4317"
            - name: VERSION
              value: ""
            - name: OPERATOR_MODE
              value: node-agent
            - name: NODE_AGENT_BIND_ADDRESS
              value: ":8090"

          image: "quay.io/weka.io/weka-operator:v1.6.0"
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /tmp
              name: tmpdir
            - mountPath: /host-binds/shared
              name: weka-persistence
            - mountPath: /var/run/secrets/kubernetes.io/token
              name: token
              readOnly: true
          name: node-agent
          securityContext:
            privileged: true
          resources:
            limits:
              cpu: 1000m
              memory: 1024Mi
            requests:
              cpu: 50m
              memory: 64Mi
      serviceAccountName: "weka-operator-maintenance"
      terminationGracePeriodSeconds: 10
---
# Source: weka-operator/templates/manager.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "weka-operator-controller-manager"
  labels:
    app.kubernetes.io/component: manager
    app.kubernetes.io/created-by: weka-operator
    app.kubernetes.io/instance: controller-manager
    app.kubernetes.io/name: deployment
    app.kubernetes.io/part-of: weka-operator
    control-plane: controller-manager
spec:
  selector:
    matchLabels:
      control-plane: controller-manager
  replicas:  1 
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
        prometheus.io/port: "8080"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
      labels:
        control-plane: controller-manager
        app.kubernetes.io/component: weka-operator
        app.kubernetes.io/created-by: weka-operator
        app: weka-operator
    spec:
      securityContext:
        runAsNonRoot: false
      imagePullSecrets:
        - name: "quay-io-robot-secret"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
                  - key: kubernetes.io/os
                    operator: In
                    values:
                      - linux
      tolerations:
      dnsPolicy: 
      volumes:
        - name: tmpdir
          emptyDir: { }
      containers:
        - args:
            - --secure-listen-address=0.0.0.0:8443
            - --upstream=http://127.0.0.1:8080/
            - --logtostderr=true
            - --v=0
          image: gcr.io/kubebuilder/kube-rbac-proxy:v0.13.1
          name: kube-rbac-proxy
          ports:
            - containerPort: 8443
              name: https
              protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
        - command:
            - /weka-operator
          ports:
              - containerPort: 8080
                name: metrics
                protocol: TCP
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_DEPLOYMENT_IDENTIFIER
              value: ""
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "https://otelcollector.rnd.weka.io:4317"
            - name: VERSION
              value: ""
            - name: WEKA_OPERATOR_WEKA_HOME_ENDPOINT
              value: "https://api.home.weka.io"
            - name: WEKA_OPERATOR_WEKA_HOME_CACERT_SECRET
              value: ""
            - name: WEKA_OPERATOR_WEKA_HOME_INSECURE
              value: "false"
            - name: WEKA_OPERATOR_WEKA_HOME_ENABLE_STATS
              value: "true"
            - name: WEKA_OPERATOR_DEBUG_SLEEP
              value: "3"
            - name: WEKA_OPERATOR_MAINTENANCE_SA_NAME
              value: "weka-operator-maintenance"
            - name: WEKA_MAINTENANCE_IMAGE
              value: "busybox"
            - name: WEKA_MAINTENANCE_IMAGE_PULL_SECRET
              value: ""
            - name: WEKA_OCP_PULL_SECRET
              value: ""
            - name: WEKA_OCP_TOOLKIT_IMAGE_BASE_URL
              value: "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
            - name: WEKA_COS_ALLOW_DISABLE_DRIVER_SIGNING
              value: "false"
            - name: WEKA_COS_ALLOW_HUGEPAGE_CONFIG
              value: "false"
            - name: WEKA_COS_GLOBAL_HUGEPAGE_SIZE
              value: "2M"
            - name: WEKA_COS_GLOBAL_HUGEPAGE_COUNT
              value: "4000"
            - name: WEKA_COS_SERVICE_ACCOUNT_SECRET
              value: ""
            - name: SKIP_UNHEALTHY_TOLERATION
              value: "false"
            - name: CLEANUP_REMOVED_NODES
              value: "false"
            - name: CLEANUP_BACKENDS_ON_NODE_SELECTOR_MISMATCH
              value: "false"
            - name: CLEANUP_CLIENTS_ON_NODE_SELECTOR_MISMATCH
              value: "false"
            - name: CLEANUP_CONTAINERS_ON_TOLERATIONS_MISMATCH
              value: "false"
            - name: LOG_LEVEL
              value: "0"
            - name: LOG_TIME_ONLY
              value: "true"
            - name: HEALTH_PROBE_BIND_ADDRESS
              value: ":8081"
            - name: OPERATOR_METRICS_BIND_ADDRESS
              value: "127.0.0.1:8080"
            - name: ENABLE_LEADER_ELECTION
              value: "true"
            - name: ENABLE_CLUSTER_API
              value: "false"
            - name: RECONCILE_TIMEOUT
              value: "30m"
            - name: KUBE_EXEC_TIMEOUT
              value: "5m"
            - name: WEKA_ALLOC_ZOMBIE_DELETE_AFTER
              value: "5m"
            - name: MAX_WORKERS_WEKACLUSTER
              value: "5"
            - name: MAX_WORKERS_WEKACONTAINER
              value: "50"
            - name: MAX_WORKERS_WEKACLIENT
              value: "5"
            - name: MAX_WORKERS_WEKAMANUALOPERATION
              value: "5"
            - name: MAX_WORKERS_WEKAPOLICY
              value: "5"
            - name: METRICS_CLUSTERS_ENABLED
              value: "true"
            - name: METRICS_CLUSTERS_POLLING_RATE
              value: "60s"
            - name: METRICS_CONTAINERS_ENABLED
              value: "true"
            - name: METRICS_CONTAINERS_POLLING_RATE
              value: "60s"
            - name: METRICS_CONTAINERS_REQUEST_TIMEOUT_REGISTER
              value: "3s"
            - name: METRICS_CONTAINERS_REQUEST_TIMEOUT_GET_CONTAINER_INFO
              value: "10s"
            - name: LOCAL_DATA_PVC
              value: ""
            - name: DNS_POLICY_K8S_NETWORK
              value: ""
            - name: DNS_POLICY_HOST_NETWORK
              value: ""
            - name: SIGN_DRIVES_IMAGE
              value: "quay.io/weka.io/weka-sign-tool:v0.1.1-pciutils"
            - name: UPGRADE_COMPUTE_THRESHOLD_PERCENT
              value: "90"
            - name: UPGRADE_DRIVE_THRESHOLD_PERCENT
              value: "90"
            - name: UPGRADE_MAX_DEACTIVATING_CONTAINERS_PERCENT
              value: "10"
            - name: SKIP_CLIENT_NO_SCHEDULE_TOLERATION
              value: "false"
            - name: SKIP_AUX_NO_SCHEDULE_TOLERATION
              value: "false"
            - name: EVICT_CONTAINER_ON_DELETION
              value: "false"
            - name: SKIP_CLIENTS_TOLERATION_VALIDATION
              value: "false"


          image: "quay.io/weka.io/weka-operator:v1.6.0"
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /tmp
              name: tmpdir
          name: manager
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - "ALL"
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          # TODO(user): Configure the resources accordingly based on the project requirements.
          # More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
          resources:
            limits:
              cpu: 1000m
              memory: 4096Mi
            requests:
              cpu: 250m
              memory: 64Mi
      serviceAccountName: "weka-operator-controller-manager"
      terminationGracePeriodSeconds: 10
---
# Source: weka-operator/templates/node_describe_role.yaml
# cluster role that allowed to perform describe ("get") over ["nodes"]

NOTES:
Chart: weka-operator
Release: weka-operator
```
